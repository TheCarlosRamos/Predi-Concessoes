{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "# Análise preditiva\n",
    "\n",
    "Siga estes passos:\n",
    "- Ajuste `TARGET_COLUMN` na próxima célula (ou deixe `None` para autodetecção).\n",
    "- Execute as células em ordem, de cima para baixo.\n",
    "- Os arquivos `.xlsx` serão lidos do diretório atual e da pasta `Dados/`.\n",
    "- Saídas: modelos em `models/` e métricas em `artifacts/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dica: execute primeiro as células de configuração e de carregamento de dados. Depois use a célula de \"Preview de colunas\" mais abaixo.\n"
     ]
    }
   ],
   "source": [
    "# Ajuda rápida: visualizar colunas e candidatos a alvo\n",
    "print('Dica: execute primeiro as células de configuração e de carregamento de dados. Depois use a célula de \"Preview de colunas\" mais abaixo.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versões:\n",
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Pandas: 2.3.2\n",
      "Scikit-learn: 1.7.2\n",
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Configuração inicial\n",
    "# Ajuste estes parâmetros conforme seu caso\n",
    "TARGET_COLUMN = 'INVESTIMENTO TOTAL (BI)'\n",
    "DATE_COLUMNS_CANDIDATES = ['data', 'dt', 'date', 'competencia', 'mes', 'mês', 'ano']\n",
    "ID_COLUMNS_CANDIDATES = ['id', 'codigo', 'código', 'cod', 'cód', 'cnpj', 'cpf']\n",
    "TASK_TYPE = 'regression'  # regressão sobre INVESTIMENTO TOTAL (BI)\n",
    "SHEET_NAME_PREFERRED = 'todas as tabelas'\n",
    "PREFERRED_COLUMNS = [\n",
    "\t'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'EXTENSÃO (km)', '(km)%', '(km)% EXEC', '(km)% PLAN',\n",
    "\t'Ext. (km)', 'FINANCEIRO (R$)', 'FINANCEIRO PLAN (R$)', 'PERCENTUAL (%)', 'PERCENTUAL (%) EXEC', 'PERCENTUAL (%) PLAN'\n",
    "]\n",
    "PRIORITY_TARGETS = ['INVESTIMENTO TOTAL (BI)', 'PERCENTUAL (%)', 'FINANCEIRO (R$)']\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "MAX_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 20\n",
    "MODEL_DIR = 'models'\n",
    "ARTIFACTS_DIR = 'artifacts'\n",
    "# Limite opcional de linhas para acelerar (None desativa)\n",
    "MAX_ROWS = None\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional installs (executa apenas se faltar pacote)\n",
    "try:\n",
    "\timport sklearn  # type: ignore\n",
    "except Exception:\n",
    "\t!pip -q install scikit-learn\n",
    "\timport sklearn  # type: ignore\n",
    "\n",
    "try:\n",
    "\timport tensorflow as tf  # type: ignore\n",
    "except Exception:\n",
    "\t!pip -q install tensorflow==2.*\n",
    "\timport tensorflow as tf  # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "print('Versões:')\n",
    "print('Python:', sys.version)\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Scikit-learn:', sklearn.__version__)\n",
    "print('TensorFlow:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregue os dados primeiro.\n"
     ]
    }
   ],
   "source": [
    "# Preferência por alvos informados e seleção automática\n",
    "if 'df_raw' in globals():\n",
    "\tif TARGET_COLUMN is None:\n",
    "\t\tfor t in PRIORITY_TARGETS:\n",
    "\t\t\tif t in df_raw.columns:\n",
    "\t\t\t\tTARGET_COLUMN = t\n",
    "\t\t\t\tprint('TARGET_COLUMN escolhido por prioridade =', TARGET_COLUMN)\n",
    "\t\t\t\tbreak\n",
    "\t\tif TARGET_COLUMN is None:\n",
    "\t\t\tprint('Nenhum alvo prioritário encontrado; manter heurística padrão.')\n",
    "else:\n",
    "\tprint('Carregue os dados primeiro.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Excel files:\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 6.xlsx\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 7.xlsx\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 8.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 1.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 2.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 3.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 6.xlsx\n",
      "Raw shape: (606526, 49)\n",
      "Columns: ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', '__source_file', '__source_sheet', 'ID-ÚNICO', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'ID-ÚNICO2', 'SETOR2', 'EMPREENDIMENTO2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Região</th>\n",
       "      <th>BR</th>\n",
       "      <th>EMPREENDIMENTO</th>\n",
       "      <th>PROPONENTE</th>\n",
       "      <th>EXECUTOR (Grupo Controlador)</th>\n",
       "      <th>ESTRUTURADOR DO PROJETO</th>\n",
       "      <th>ANO LEILÃO</th>\n",
       "      <th>DATA DE INÍCIO</th>\n",
       "      <th>ANO DA CONCESSÃO</th>\n",
       "      <th>PRAZO (anos)</th>\n",
       "      <th>...</th>\n",
       "      <th>(km)% PLAN</th>\n",
       "      <th>Descrição</th>\n",
       "      <th>Ext. (km)</th>\n",
       "      <th>FINANCEIRO (R$)</th>\n",
       "      <th>FINANCEIRO PLAN (R$)</th>\n",
       "      <th>PERCENTUAL (%)</th>\n",
       "      <th>PERCENTUAL (%) EXEC</th>\n",
       "      <th>PERCENTUAL (%) PLAN</th>\n",
       "      <th>km (f)</th>\n",
       "      <th>km (i)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VIA SUL</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>CCR</td>\n",
       "      <td>EPL</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD/CO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CERRADO</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>ECORODOVIAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VIA COSTEIRA</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>CCR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Região   BR EMPREENDIMENTO PROPONENTE EXECUTOR (Grupo Controlador)  \\\n",
       "0    SUL  1.0        VIA SUL       ANTT                          CCR   \n",
       "1  SD/CO  1.0        CERRADO       ANTT                  ECORODOVIAS   \n",
       "2    SUL  1.0   VIA COSTEIRA       ANTT                          CCR   \n",
       "\n",
       "  ESTRUTURADOR DO PROJETO  ANO LEILÃO DATA DE INÍCIO  ANO DA CONCESSÃO  \\\n",
       "0                     EPL      2018.0     2019-02-15               7.0   \n",
       "1                     NaN      2019.0     2020-01-20               6.0   \n",
       "2                     NaN      2020.0     2020-07-08               5.0   \n",
       "\n",
       "   PRAZO (anos)  ...  (km)% PLAN  Descrição  Ext. (km) FINANCEIRO (R$)  \\\n",
       "0          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "1          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "2          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "\n",
       "  FINANCEIRO PLAN (R$)  PERCENTUAL (%) PERCENTUAL (%) EXEC  \\\n",
       "0                  NaN             NaN                 NaN   \n",
       "1                  NaN             NaN                 NaN   \n",
       "2                  NaN             NaN                 NaN   \n",
       "\n",
       "  PERCENTUAL (%) PLAN km (f) km (i)  \n",
       "0                 NaN    NaN    NaN  \n",
       "1                 NaN    NaN    NaN  \n",
       "2                 NaN    NaN    NaN  \n",
       "\n",
       "[3 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Discover and load Excel files\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path('.')\n",
    "DATA_DIRS = [ROOT, ROOT / 'Dados']\n",
    "excel_files = []\n",
    "for d in DATA_DIRS:\n",
    "\tif d.exists():\n",
    "\t\texcel_files.extend(sorted(map(Path, glob.glob(str(d / '*.xlsx')))))\n",
    "\n",
    "if not excel_files:\n",
    "\traise FileNotFoundError('No .xlsx files found in project root or Dados/.')\n",
    "\n",
    "print(f'Found {len(excel_files)} Excel files:')\n",
    "for p in excel_files:\n",
    "\tprint('-', p)\n",
    "\n",
    "# Load preferred sheet if present, otherwise all sheets\n",
    "frames = []\n",
    "for fp in excel_files:\n",
    "\ttry:\n",
    "\t\txl = pd.ExcelFile(fp)\n",
    "\t\tsheets = xl.sheet_names\n",
    "\t\tif SHEET_NAME_PREFERRED in sheets:\n",
    "\t\t\tcandidate_sheets = [SHEET_NAME_PREFERRED]\n",
    "\t\telse:\n",
    "\t\t\tcandidate_sheets = sheets\n",
    "\t\tfor sheet in candidate_sheets:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf = xl.parse(sheet)\n",
    "\t\t\t\tdf['__source_file'] = fp.name\n",
    "\t\t\t\tdf['__source_sheet'] = sheet\n",
    "\t\t\t\tframes.append(df)\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'Warning parsing sheet {sheet} in {fp.name}: {e}')\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Warning opening {fp.name}: {e}')\n",
    "\n",
    "if not frames:\n",
    "\traise RuntimeError('Could not read any sheet from Excel files.')\n",
    "\n",
    "df_raw = pd.concat(frames, ignore_index=True)\n",
    "print('Raw shape:', df_raw.shape)\n",
    "print('Columns:', list(df_raw.columns)[:30])\n",
    "\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas (primeiras 30): ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', '__source_file', '__source_sheet', 'ID-ÚNICO', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'ID-ÚNICO2', 'SETOR2', 'EMPREENDIMENTO2']\n",
      "Tipos:\n",
      "Região                                  object\n",
      "BR                                     float64\n",
      "EMPREENDIMENTO                          object\n",
      "PROPONENTE                              object\n",
      "EXECUTOR (Grupo Controlador)            object\n",
      "ESTRUTURADOR DO PROJETO                 object\n",
      "ANO LEILÃO                             float64\n",
      "DATA DE INÍCIO                  datetime64[ns]\n",
      "ANO DA CONCESSÃO                       float64\n",
      "PRAZO (anos)                           float64\n",
      "CAPEX (BI)                             float64\n",
      "OPEX (BI)                              float64\n",
      "INVESTIMENTO TOTAL (BI)                float64\n",
      "ETAPA                                   object\n",
      "SITUAÇÃO                                object\n",
      "EXTENSÃO (km)                          float64\n",
      "ESTADO/LOTE                             object\n",
      "PLANILHA                                object\n",
      "__source_file                           object\n",
      "__source_sheet                          object\n",
      "ID-ÚNICO                               float64\n",
      "SETOR                                   object\n",
      "UF                                      object\n",
      "Atributo.1                              object\n",
      "Atributo.2                              object\n",
      "Atributo.3                              object\n",
      "Valor                                   object\n",
      "ID-ÚNICO2                              float64\n",
      "SETOR2                                  object\n",
      "EMPREENDIMENTO2                         object\n",
      "dtype: object\n",
      "Possíveis alvos numéricos: ['BR', 'ANO LEILÃO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'EXTENSÃO (km)', 'ID-ÚNICO', 'ID-ÚNICO2']\n",
      "Possíveis alvos categóricos: ['Região', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'DATA DE INÍCIO', 'ETAPA', 'SITUAÇÃO', 'ESTADO/LOTE', 'PLANILHA']\n"
     ]
    }
   ],
   "source": [
    "# Preview de colunas (execute após carregar df_raw)\n",
    "if 'df_raw' not in globals():\n",
    "\tprint('Carregue os dados primeiro.')\n",
    "else:\n",
    "\tprint('Colunas (primeiras 30):', list(df_raw.columns)[:30])\n",
    "\tprint('Tipos:')\n",
    "\tprint(df_raw.dtypes.head(30))\n",
    "\tnum_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "\tcat_cols = [c for c in df_raw.columns if c not in num_cols]\n",
    "\tprint('Possíveis alvos numéricos:', num_cols[:10])\n",
    "\tprint('Possíveis alvos categóricos:', cat_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correção BR aplicada onde possível nas colunas preferidas.\n"
     ]
    }
   ],
   "source": [
    "# Correção de formatos BR para colunas preferidas\n",
    "# Remove R$, pontos de milhar, troca vírgula por ponto e trata %\n",
    "\n",
    "def br_to_float(series: pd.Series) -> pd.Series:\n",
    "\tif pd.api.types.is_numeric_dtype(series):\n",
    "\t\treturn series\n",
    "\ts = series.astype(str).str.strip()\n",
    "\ts = s.str.replace(r'\\s', '', regex=True)\n",
    "\ts = s.str.replace('R$', '', regex=False)\n",
    "\ts = s.str.replace('.', '', regex=False)\n",
    "\ts = s.str.replace(',', '.', regex=False)\n",
    "\t# Percentuais: transformar \"10%\" em 0.10\n",
    "\tis_percent = s.str.contains('%', regex=False)\n",
    "\ts = s.str.replace('%', '', regex=False)\n",
    "\tout = pd.to_numeric(s, errors='coerce')\n",
    "\tout = out.where(~is_percent, out / 100.0)\n",
    "\treturn out\n",
    "\n",
    "if 'df_raw' in globals():\n",
    "\tfor col in PREFERRED_COLUMNS:\n",
    "\t\tif col in df_raw.columns:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf_raw[col] = br_to_float(df_raw[col])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'Falha ao converter {col}: {e}')\n",
    "\tprint('Correção BR aplicada onde possível nas colunas preferidas.')\n",
    "else:\n",
    "\tprint('Carregue os dados primeiro.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate date columns: ['ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)']\n",
      "Candidate id columns: ['ID-ÚNICO', 'ID-ÚNICO2', 'ID-ÚNICO-TT']\n"
     ]
    }
   ],
   "source": [
    "# Infer target, date and id columns helpers\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "\treturn str(name).strip().lower().replace('\\n',' ').replace('\\r',' ')\n",
    "\n",
    "norm_cols = {c: normalize_name(c) for c in df_raw.columns}\n",
    "\n",
    "# Try to infer target if not provided\n",
    "if TARGET_COLUMN is None:\n",
    "\t# Heuristic: last numeric column\n",
    "\tnumeric_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "\tTARGET_COLUMN = numeric_cols[-1] if numeric_cols else df_raw.columns[-1]\n",
    "\tprint('Auto-selected TARGET_COLUMN =', TARGET_COLUMN)\n",
    "else:\n",
    "\tassert TARGET_COLUMN in df_raw.columns, f'TARGET_COLUMN {TARGET_COLUMN} not in data.'\n",
    "\n",
    "# Detect date-like columns\n",
    "candidate_date_cols = []\n",
    "for c, n in norm_cols.items():\n",
    "\tif any(tok in n for tok in DATE_COLUMNS_CANDIDATES):\n",
    "\t\tcandidate_date_cols.append(c)\n",
    "\n",
    "# Also include truly datetime types\n",
    "for c in df_raw.columns:\n",
    "\tif pd.api.types.is_datetime64_any_dtype(df_raw[c]):\n",
    "\t\tcandidate_date_cols.append(c)\n",
    "\n",
    "candidate_date_cols = list(dict.fromkeys(candidate_date_cols))  # unique\n",
    "print('Candidate date columns:', candidate_date_cols)\n",
    "\n",
    "# Detect id-like columns\n",
    "candidate_id_cols = []\n",
    "for c, n in norm_cols.items():\n",
    "\tif any(tok in n for tok in ID_COLUMNS_CANDIDATES):\n",
    "\t\tcandidate_id_cols.append(c)\n",
    "print('Candidate id columns:', candidate_id_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time column: ANO LEILÃO\n",
      "Feature columns (first 30): ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'SETOR2', 'EMPREENDIMENTO2', 'Descrição2', 'Ext.(km)2', 'FINANCEIRO PLAN (R$)2', 'FINANCEIRO(R$)2', '(km)% PLAN2']\n",
      "X shape: (406, 47) y shape: (406,)\n",
      "TASK_TYPE = regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Build feature table\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Clean target FIRST: coerce to numeric and drop NaN/inf\n",
    "if TARGET_COLUMN in df.columns:\n",
    "\tdf[TARGET_COLUMN] = pd.to_numeric(df[TARGET_COLUMN], errors='coerce')\n",
    "\tmask_target = df[TARGET_COLUMN].apply(lambda v: pd.notna(v) and np.isfinite(v))\n",
    "\tdf = df[mask_target]\n",
    "\n",
    "# Stop early if no rows remain\n",
    "if len(df) == 0:\n",
    "\traise SystemExit('Sem linhas com alvo válido após limpeza do alvo.')\n",
    "\n",
    "# Parse first date column if available\n",
    "main_date_col = None\n",
    "for c in candidate_date_cols:\n",
    "\ttry:\n",
    "\t\tdf[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
    "\t\tif df[c].notna().sum() > 0 and main_date_col is None:\n",
    "\t\t\tmain_date_col = c\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\n",
    "# Optional row limit to speed up (apply after knowing main_date_col)\n",
    "if 'MAX_ROWS' in globals() and MAX_ROWS is not None:\n",
    "\tif main_date_col and main_date_col in df.columns:\n",
    "\t\tdf = df.sort_values(by=main_date_col).tail(int(MAX_ROWS))\n",
    "\telse:\n",
    "\t\tdf = df.sample(n=int(MAX_ROWS), random_state=RANDOM_STATE) if len(df) > MAX_ROWS else df\n",
    "\n",
    "if main_date_col:\n",
    "\tdf['__year'] = df[main_date_col].dt.year\n",
    "\tdf['__month'] = df[main_date_col].dt.month\n",
    "\tdf['__day'] = df[main_date_col].dt.day\n",
    "\tdf['__quarter'] = df[main_date_col].dt.quarter\n",
    "\tprint('Using time column:', main_date_col)\n",
    "else:\n",
    "\tprint('No usable time column found; will use random split.')\n",
    "\n",
    "# Drop purely identifier columns and technical columns\n",
    "cols_to_drop = set(candidate_id_cols + ['__source_file', '__source_sheet'])\n",
    "feature_cols = [c for c in df.columns if c not in cols_to_drop and c != TARGET_COLUMN]\n",
    "\n",
    "# Separate X, y\n",
    "X = df[feature_cols]\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "print('Feature columns (first 30):', feature_cols[:30])\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "\n",
    "# Determine task type\n",
    "if TASK_TYPE is None:\n",
    "\tif pd.api.types.is_numeric_dtype(y):\n",
    "\t\tunique_vals = pd.Series(y).nunique(dropna=True)\n",
    "\t\tTASK_TYPE = 'regression' if unique_vals > 15 else 'classification'\n",
    "\telse:\n",
    "\t\tTASK_TYPE = 'classification'\n",
    "print('TASK_TYPE =', TASK_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 22 Categorical features: 25\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "# Ensure categorical columns are uniformly strings (preserve NaN)\n",
    "def to_str_preserve_nan(df_in):\n",
    "\treturn df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "\t('imputer_median', SimpleImputer(strategy='median')),\n",
    "\t('imputer_zero', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "\t('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "\t('to_str', FunctionTransformer(to_str_preserve_nan)),\n",
    "\t('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "\t('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=0.01)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', numeric_pipeline, numeric_features),\n",
    "\t\t('cat', categorical_pipeline, categorical_features),\n",
    "\t],\n",
    "\tremainder='drop',\n",
    ")\n",
    "\n",
    "print('Numeric features:', len(numeric_features), 'Categorical features:', len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (330, 47) Val: (36, 47) Test: (40, 47)\n",
      "Transformed dims: (330, 142) (36, 142) (40, 142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation/Test split (time-aware if possible)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if main_date_col:\n",
    "\t# Sort by time and split by index ratios\n",
    "\tdf_sorted = df.sort_values(by=main_date_col)\n",
    "\tX_sorted = df_sorted[feature_cols]\n",
    "\ty_sorted = df_sorted[TARGET_COLUMN]\n",
    "\t\n",
    "\tn_total = len(df_sorted)\n",
    "\tn_test = max(1, int(math.floor(TEST_SIZE * n_total))) if n_total >= 3 else 1\n",
    "\tn_val_pool = max(0, n_total - n_test)\n",
    "\tn_val = max(1, int(math.floor(VAL_SIZE * n_val_pool))) if n_val_pool >= 3 else (1 if n_val_pool >= 2 else 0)\n",
    "\ttrain_end = n_total - n_test - n_val\n",
    "\tif train_end <= 0:\n",
    "\t\t# Fallback to random split to avoid empty sets\n",
    "\t\tX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=min(0.2, max(0.1, TEST_SIZE)), random_state=RANDOM_STATE, stratify=(y if TASK_TYPE=='classification' else None))\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=min(0.2, max(0.1, VAL_SIZE)), random_state=RANDOM_STATE, stratify=(y_temp if TASK_TYPE=='classification' else None))\n",
    "\telse:\n",
    "\t\tX_train = X_sorted.iloc[: train_end]\n",
    "\t\ty_train = y_sorted.iloc[: train_end]\n",
    "\t\tX_val = X_sorted.iloc[train_end : train_end + n_val]\n",
    "\t\ty_val = y_sorted.iloc[train_end : train_end + n_val]\n",
    "\t\tX_test = X_sorted.iloc[n_total - n_test :]\n",
    "\t\ty_test = y_sorted.iloc[n_total - n_test :]\n",
    "else:\n",
    "\tX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=(y if TASK_TYPE=='classification' else None))\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=(y_temp if TASK_TYPE=='classification' else None))\n",
    "\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Guard against empty splits\n",
    "assert len(X_train) > 0 and len(X_val) > 0 and len(X_test) > 0, 'One of the splits is empty after splitting.'\n",
    "\n",
    "# Fit preprocessor on train only\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "input_dim = X_train_t.shape[1]\n",
    "print('Transformed dims:', X_train_t.shape, X_val_t.shape, X_test_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 46ms/step - loss: 10.6064 - mae: 10.6064 - mse: 167.4851 - val_loss: 10.0564 - val_mae: 10.0564 - val_mse: 109.7840\n",
      "Epoch 2/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10.5747 - mae: 10.5747 - mse: 154.3349 - val_loss: 9.8936 - val_mae: 9.8936 - val_mse: 106.3346\n",
      "Epoch 3/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10.4518 - mae: 10.4518 - mse: 145.2920 - val_loss: 9.7349 - val_mae: 9.7349 - val_mse: 103.1143\n",
      "Epoch 4/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 10.2575 - mae: 10.2575 - mse: 141.0740 - val_loss: 9.5872 - val_mae: 9.5872 - val_mse: 100.2622\n",
      "Epoch 5/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 10.1769 - mae: 10.1769 - mse: 140.8357 - val_loss: 9.4567 - val_mae: 9.4567 - val_mse: 97.8438\n",
      "Epoch 6/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 10.0800 - mae: 10.0800 - mse: 138.8661 - val_loss: 9.3062 - val_mae: 9.3062 - val_mse: 95.0346\n",
      "Epoch 7/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.9468 - mae: 9.9468 - mse: 136.3126 - val_loss: 9.1611 - val_mae: 9.1611 - val_mse: 92.3791\n",
      "Epoch 8/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.8439 - mae: 9.8439 - mse: 133.9183 - val_loss: 8.9936 - val_mae: 8.9936 - val_mse: 89.2725\n",
      "Epoch 9/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.7009 - mae: 9.7009 - mse: 130.3521 - val_loss: 8.8150 - val_mae: 8.8150 - val_mse: 85.9408\n",
      "Epoch 10/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.5404 - mae: 9.5404 - mse: 123.9874 - val_loss: 8.6285 - val_mae: 8.6285 - val_mse: 82.6306\n",
      "Epoch 11/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.3267 - mae: 9.3267 - mse: 119.1097 - val_loss: 8.4606 - val_mae: 8.4606 - val_mse: 79.7254\n",
      "Epoch 12/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 9.2385 - mae: 9.2385 - mse: 117.4028 - val_loss: 8.2631 - val_mae: 8.2631 - val_mse: 76.3220\n",
      "Epoch 13/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.9561 - mae: 8.9561 - mse: 109.0839 - val_loss: 8.0389 - val_mae: 8.0389 - val_mse: 72.4461\n",
      "Epoch 14/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.8141 - mae: 8.8141 - mse: 106.9610 - val_loss: 7.7709 - val_mae: 7.7709 - val_mse: 67.8039\n",
      "Epoch 15/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 8.5815 - mae: 8.5815 - mse: 100.4530 - val_loss: 7.4955 - val_mae: 7.4955 - val_mse: 63.2740\n",
      "Epoch 16/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.3109 - mae: 8.3109 - mse: 94.1025 - val_loss: 7.2298 - val_mae: 7.2298 - val_mse: 59.1016\n",
      "Epoch 17/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.1080 - mae: 8.1080 - mse: 88.1178 - val_loss: 6.9303 - val_mae: 6.9303 - val_mse: 54.5561\n",
      "Epoch 18/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.7209 - mae: 7.7209 - mse: 81.5632 - val_loss: 6.6105 - val_mae: 6.6105 - val_mse: 50.0274\n",
      "Epoch 19/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.4429 - mae: 7.4429 - mse: 78.6265 - val_loss: 6.2737 - val_mae: 6.2737 - val_mse: 45.3361\n",
      "Epoch 20/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.1192 - mae: 7.1192 - mse: 71.9995 - val_loss: 5.9463 - val_mae: 5.9463 - val_mse: 40.8619\n",
      "Epoch 21/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.7806 - mae: 6.7806 - mse: 64.4140 - val_loss: 5.6498 - val_mae: 5.6498 - val_mse: 37.4353\n",
      "Epoch 22/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 6.4666 - mae: 6.4666 - mse: 60.1437 - val_loss: 5.3253 - val_mae: 5.3253 - val_mse: 34.0961\n",
      "Epoch 23/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.9286 - mae: 5.9286 - mse: 53.1312 - val_loss: 4.8811 - val_mae: 4.8811 - val_mse: 29.3637\n",
      "Epoch 24/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.4504 - mae: 5.4504 - mse: 46.8807 - val_loss: 4.3286 - val_mae: 4.3286 - val_mse: 23.4710\n",
      "Epoch 25/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.9456 - mae: 4.9456 - mse: 39.3361 - val_loss: 3.9455 - val_mae: 3.9455 - val_mse: 19.4801\n",
      "Epoch 26/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 4.4357 - mae: 4.4357 - mse: 30.7656 - val_loss: 3.8151 - val_mae: 3.8151 - val_mse: 19.2696\n",
      "Epoch 27/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.1773 - mae: 4.1773 - mse: 26.5041 - val_loss: 3.4112 - val_mae: 3.4112 - val_mse: 15.1697\n",
      "Epoch 28/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.7966 - mae: 3.7966 - mse: 20.6819 - val_loss: 3.1721 - val_mae: 3.1721 - val_mse: 12.9612\n",
      "Epoch 29/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.4001 - mae: 3.4001 - mse: 19.1339 - val_loss: 2.8039 - val_mae: 2.8039 - val_mse: 9.5202\n",
      "Epoch 30/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.1358 - mae: 3.1358 - mse: 17.4582 - val_loss: 2.3651 - val_mae: 2.3651 - val_mse: 6.9735\n",
      "Epoch 31/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.7920 - mae: 2.7920 - mse: 13.6050 - val_loss: 1.8890 - val_mae: 1.8890 - val_mse: 5.1190\n",
      "Epoch 32/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.4672 - mae: 2.4672 - mse: 11.3116 - val_loss: 1.5032 - val_mae: 1.5032 - val_mse: 4.1493\n",
      "Epoch 33/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.4650 - mae: 2.4650 - mse: 9.3114 - val_loss: 1.3784 - val_mae: 1.3784 - val_mse: 3.0435\n",
      "Epoch 34/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.0218 - mae: 2.0218 - mse: 6.7701 - val_loss: 1.4056 - val_mae: 1.4056 - val_mse: 3.3290\n",
      "Epoch 35/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.0888 - mae: 2.0888 - mse: 7.2098 - val_loss: 1.2755 - val_mae: 1.2755 - val_mse: 2.6476\n",
      "Epoch 36/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1084 - mae: 2.1084 - mse: 8.3629 - val_loss: 1.0655 - val_mae: 1.0655 - val_mse: 2.4030\n",
      "Epoch 37/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.0598 - mae: 2.0598 - mse: 7.4160 - val_loss: 1.2597 - val_mae: 1.2597 - val_mse: 3.1640\n",
      "Epoch 38/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8049 - mae: 1.8049 - mse: 6.3559 - val_loss: 1.1226 - val_mae: 1.1226 - val_mse: 2.6107\n",
      "Epoch 39/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9455 - mae: 1.9455 - mse: 6.4285 - val_loss: 1.2496 - val_mae: 1.2496 - val_mse: 3.0665\n",
      "Epoch 40/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6628 - mae: 1.6628 - mse: 4.7354 - val_loss: 1.2770 - val_mae: 1.2770 - val_mse: 3.2959\n",
      "Epoch 41/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7012 - mae: 1.7012 - mse: 4.8078 - val_loss: 1.2036 - val_mae: 1.2036 - val_mse: 2.9576\n",
      "Epoch 42/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5555 - mae: 1.5555 - mse: 4.4240 - val_loss: 1.1000 - val_mae: 1.1000 - val_mse: 2.6657\n",
      "Epoch 43/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7063 - mae: 1.7063 - mse: 5.0034 - val_loss: 1.1223 - val_mae: 1.1223 - val_mse: 2.4505\n",
      "Epoch 44/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.8004 - mae: 1.8004 - mse: 5.4107 - val_loss: 0.9591 - val_mae: 0.9591 - val_mse: 2.1178\n",
      "Epoch 45/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 2.0726 - mae: 2.0726 - mse: 7.1463 - val_loss: 0.9136 - val_mae: 0.9136 - val_mse: 1.6348\n",
      "Epoch 46/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7399 - mae: 1.7399 - mse: 4.8666 - val_loss: 0.8225 - val_mae: 0.8225 - val_mse: 1.4451\n",
      "Epoch 47/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.9040 - mae: 1.9040 - mse: 5.7184 - val_loss: 0.9095 - val_mae: 0.9095 - val_mse: 1.7512\n",
      "Epoch 48/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5165 - mae: 1.5165 - mse: 3.8715 - val_loss: 0.9497 - val_mae: 0.9497 - val_mse: 1.5833\n",
      "Epoch 49/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5440 - mae: 1.5440 - mse: 4.0901 - val_loss: 0.9752 - val_mae: 0.9752 - val_mse: 1.5436\n",
      "Epoch 50/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7348 - mae: 1.7348 - mse: 4.7430 - val_loss: 1.0856 - val_mae: 1.0856 - val_mse: 1.9615\n",
      "Epoch 51/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5083 - mae: 1.5083 - mse: 3.7023 - val_loss: 0.7687 - val_mae: 0.7687 - val_mse: 1.2724\n",
      "Epoch 52/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6454 - mae: 1.6454 - mse: 4.4639 - val_loss: 0.8552 - val_mae: 0.8552 - val_mse: 1.5466\n",
      "Epoch 53/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5794 - mae: 1.5794 - mse: 4.0816 - val_loss: 0.8331 - val_mae: 0.8331 - val_mse: 1.3564\n",
      "Epoch 54/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4836 - mae: 1.4836 - mse: 3.7013 - val_loss: 0.9531 - val_mae: 0.9531 - val_mse: 1.4873\n",
      "Epoch 55/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4523 - mae: 1.4523 - mse: 3.6907 - val_loss: 1.0978 - val_mae: 1.0978 - val_mse: 2.0783\n",
      "Epoch 56/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7686 - mae: 1.7686 - mse: 5.7108 - val_loss: 1.1923 - val_mae: 1.1923 - val_mse: 2.3421\n",
      "Epoch 57/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7422 - mae: 1.7422 - mse: 4.9547 - val_loss: 1.0324 - val_mae: 1.0324 - val_mse: 1.6083\n",
      "Epoch 58/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.7216 - mae: 1.7216 - mse: 4.8911 - val_loss: 0.9438 - val_mae: 0.9438 - val_mse: 1.2759\n",
      "Epoch 59/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4606 - mae: 1.4606 - mse: 3.5401 - val_loss: 1.3182 - val_mae: 1.3182 - val_mse: 1.9583\n",
      "Epoch 60/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5647 - mae: 1.5647 - mse: 4.3741 - val_loss: 1.2961 - val_mae: 1.2961 - val_mse: 1.9308\n",
      "Epoch 61/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4827 - mae: 1.4827 - mse: 4.0351 - val_loss: 1.0401 - val_mae: 1.0401 - val_mse: 1.4818\n",
      "Epoch 62/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5222 - mae: 1.5222 - mse: 3.9431 - val_loss: 0.9256 - val_mae: 0.9256 - val_mse: 1.3447\n",
      "Epoch 63/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.3768 - mae: 1.3768 - mse: 3.3581 - val_loss: 0.9588 - val_mae: 0.9588 - val_mse: 1.3087\n",
      "Epoch 64/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4615 - mae: 1.4615 - mse: 3.6882 - val_loss: 1.1767 - val_mae: 1.1767 - val_mse: 1.6622\n",
      "Epoch 65/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.5168 - mae: 1.5168 - mse: 3.7641 - val_loss: 1.5070 - val_mae: 1.5070 - val_mse: 2.5078\n",
      "Epoch 66/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.3795 - mae: 1.3795 - mse: 3.1582 - val_loss: 1.1579 - val_mae: 1.1579 - val_mse: 1.6741\n",
      "Epoch 67/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7361 - mae: 1.7361 - mse: 5.1205 - val_loss: 1.1193 - val_mae: 1.1193 - val_mse: 1.6380\n",
      "Epoch 68/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.3131 - mae: 1.3131 - mse: 3.0339 - val_loss: 1.1074 - val_mae: 1.1074 - val_mse: 1.6361\n",
      "Epoch 69/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6285 - mae: 1.6285 - mse: 4.5058 - val_loss: 1.3640 - val_mae: 1.3640 - val_mse: 2.0369\n",
      "Epoch 70/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.6005 - mae: 1.6005 - mse: 4.0942 - val_loss: 1.7252 - val_mae: 1.7252 - val_mse: 3.2943\n",
      "Epoch 71/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 1.4823 - mae: 1.4823 - mse: 3.8250 - val_loss: 1.3756 - val_mae: 1.3756 - val_mse: 2.4457\n",
      "Best val metrics: 0.7686788439750671\n"
     ]
    }
   ],
   "source": [
    "# Define and train Keras model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_mlp(input_dim: int, task: str) -> keras.Model:\n",
    "\tunits = max(16, min(512, int(2 * math.sqrt(input_dim + 1)) * 16))\n",
    "\tinputs = keras.Input(shape=(input_dim,))\n",
    "\tx = layers.Dense(units, activation='relu')(inputs)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\tx = layers.Dropout(0.2)(x)\n",
    "\tx = layers.Dense(units // 2, activation='relu')(x)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\tx = layers.Dropout(0.2)(x)\n",
    "\tif task == 'regression':\n",
    "\t\toutputs = layers.Dense(1, activation='linear')(x)\n",
    "\t\tloss = 'mae'\n",
    "\t\tmetrics = ['mae', 'mse']\n",
    "\telse:\n",
    "\t\t# detect classes\n",
    "\t\tif pd.api.types.is_numeric_dtype(y_train):\n",
    "\t\t\tclasses = np.unique(y_train.dropna())\n",
    "\t\t\tnum_classes = len(classes)\n",
    "\t\telse:\n",
    "\t\t\tclasses = np.unique(pd.Series(y_train).astype(str))\n",
    "\t\t\tnum_classes = len(classes)\n",
    "\t\toutputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\t\tloss = 'sparse_categorical_crossentropy'\n",
    "\t\tmetrics = ['accuracy']\n",
    "\tmodel = keras.Model(inputs, outputs)\n",
    "\tmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=loss, metrics=metrics)\n",
    "\treturn model\n",
    "\n",
    "model = build_mlp(input_dim, TASK_TYPE)\n",
    "\n",
    "callbacks = [\n",
    "\tkeras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Prepare targets\n",
    "if TASK_TYPE == 'classification':\n",
    "\t# Map labels to integers for sparse_categorical_crossentropy\n",
    "\tlabel_series = pd.Series(y_train)\n",
    "\tlabel_to_index = {label: idx for idx, label in enumerate(sorted(label_series.dropna().unique(), key=lambda x: str(x)))}\n",
    "\tindex_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "y_train_arr = y_train.map(label_to_index).values if TASK_TYPE=='classification' else y_train.values\n",
    "\n",
    "y_val_arr = y_val.map(label_to_index).values if TASK_TYPE=='classification' else y_val.values\n",
    "\n",
    "y_test_arr = y_test.map(label_to_index).values if TASK_TYPE=='classification' else y_test.values\n",
    "\n",
    "history = model.fit(\n",
    "\tX_train_t, y_train_arr,\n",
    "\tvalidation_data=(X_val_t, y_val_arr),\n",
    "\tepochs=MAX_EPOCHS,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tverbose=1,\n",
    "\tcallbacks=callbacks,\n",
    ")\n",
    "\n",
    "print('Best val metrics:', min(history.history['val_loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'mae': 1.0935167293548589, 'r2': 0.7676028264513364}\n",
      "Saved:\n",
      "- models\\model_regression_20250923-110842.keras\n",
      "- models\\preprocessor_20250923-110842.joblib\n",
      "- artifacts\\metrics_regression_20250923-110842.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save artifacts\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Predictions with NaN-safe evaluation\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = model.predict(X_test_t, verbose=0).reshape(-1)\n",
    "\ty_true_np = y_test.to_numpy()\n",
    "\tmask = (~np.isnan(y_true_np)) & (~np.isnan(y_pred))\n",
    "\tif mask.sum() == 0:\n",
    "\t\traise ValueError('No valid test rows to evaluate after NaN filtering.')\n",
    "\tmae_val = float(mean_absolute_error(y_true_np[mask], y_pred[mask]))\n",
    "\tr2_val = float(r2_score(y_true_np[mask], y_pred[mask]))\n",
    "\tmetrics_out = {'mae': mae_val, 'r2': r2_val}\n",
    "else:\n",
    "\tproba = model.predict(X_test_t, verbose=0)\n",
    "\ty_pred_idx = proba.argmax(axis=1)\n",
    "\tmask = ~pd.isna(y_test)\n",
    "\tif mask.sum() == 0:\n",
    "\t\traise ValueError('No valid test rows to evaluate after NaN filtering (classification).')\n",
    "\ty_true = y_test[mask]\n",
    "\ty_pred = pd.Series(y_pred_idx)[mask.to_numpy()].map(index_to_label)\n",
    "\tmetrics_out = {\n",
    "\t\t'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "\t\t'f1_macro': float(f1_score(y_true, y_pred, average='macro')),\n",
    "\t}\n",
    "\tprint(classification_report(y_true, y_pred))\n",
    "\n",
    "print('Test metrics:', metrics_out)\n",
    "\n",
    "# Save artifacts\n",
    "run_id = time.strftime('%Y%m%d-%H%M%S')\n",
    "model_path = Path(MODEL_DIR) / f'model_{TASK_TYPE}_{run_id}.keras'\n",
    "preproc_path = Path(MODEL_DIR) / f'preprocessor_{run_id}.joblib'\n",
    "report_path = Path(ARTIFACTS_DIR) / f'metrics_{TASK_TYPE}_{run_id}.json'\n",
    "labelmap_path = Path(MODEL_DIR) / f'labelmap_{run_id}.json'\n",
    "\n",
    "model.save(model_path)\n",
    "joblib.dump(preprocessor, preproc_path)\n",
    "\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(metrics_out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if TASK_TYPE == 'classification':\n",
    "\twith open(labelmap_path, 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump({'label_to_index': {str(k): int(v) for k, v in label_to_index.items()}, 'index_to_label': {str(k): str(v) for k, v in index_to_label.items()}}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print('-', model_path)\n",
    "print('-', preproc_path)\n",
    "print('-', report_path)\n",
    "if TASK_TYPE == 'classification':\n",
    "\tprint('-', labelmap_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction output shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example: inference on new data row\n",
    "# Provide a single-row DataFrame with same columns as X\n",
    "example = X.head(1).copy()\n",
    "X_new_t = preprocessor.transform(example)\n",
    "proba_or_pred = model.predict(X_new_t, verbose=0)\n",
    "print('Prediction output shape:', proba_or_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando modelo: models\\model_regression_20250923-110842.keras\n",
      "Usando preprocessor: models\\preprocessor_20250923-110842.joblib\n",
      "INPUT_PATH não definido; usando amostra de X.head(100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições salvas em: artifacts\\predicoes.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Inference: carregar dados novos e salvar predicoes.xlsx\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "# Parâmetros\n",
    "INPUT_PATH = None  # ex.: 'novos_dados.xlsx' ou 'novos_dados.csv'; se None, usa X.head(100)\n",
    "OUTPUT_PATH = 'predicoes.xlsx'\n",
    "\n",
    "# Localizar últimos artifacts gerados\n",
    "models_dir = Path(MODEL_DIR)\n",
    "artifacts_dir = Path(ARTIFACTS_DIR)\n",
    "latest_model = sorted(models_dir.glob('model_regression_*.keras') if TASK_TYPE=='regression' else models_dir.glob('model_classification_*.keras'))[-1]\n",
    "latest_preproc = sorted(models_dir.glob('preprocessor_*.joblib'))[-1]\n",
    "\n",
    "print('Usando modelo:', latest_model)\n",
    "print('Usando preprocessor:', latest_preproc)\n",
    "\n",
    "preprocessor = joblib.load(latest_preproc)\n",
    "model = keras.models.load_model(latest_model)\n",
    "\n",
    "# Carregar dados novos\n",
    "if INPUT_PATH is None:\n",
    "\tprint('INPUT_PATH não definido; usando amostra de X.head(100).')\n",
    "\tdf_new = X.head(100).copy()\n",
    "else:\n",
    "\tp = Path(INPUT_PATH)\n",
    "\tif p.suffix.lower() in ['.xlsx', '.xls']:\n",
    "\t\tdf_new = pd.read_excel(p)\n",
    "\telif p.suffix.lower() == '.csv':\n",
    "\t\tdf_new = pd.read_csv(p)\n",
    "\telse:\n",
    "\t\traise ValueError('Formato não suportado: use .xlsx, .xls ou .csv')\n",
    "\n",
    "# Garantir mesmas colunas de treino (faltantes serão criadas vazias; extras serão ignoradas pelo preprocessor)\n",
    "for c in [col for col in X.columns if col not in df_new.columns]:\n",
    "\tdf_new[c] = pd.NA\n",
    "\n",
    "X_new_t = preprocessor.transform(df_new[X.columns])\n",
    "proba_or_pred = model.predict(X_new_t, verbose=0)\n",
    "\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = proba_or_pred.reshape(-1)\n",
    "\tdf_out = df_new.copy()\n",
    "\tdf_out['pred_'+str(TARGET_COLUMN).replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')] = y_pred\n",
    "else:\n",
    "\tpred_idx = proba_or_pred.argmax(axis=1)\n",
    "\t# Recarregar labelmap mais recente, se existir\n",
    "\tlabelmaps = sorted(models_dir.glob('labelmap_*.json'))\n",
    "\tif labelmaps:\n",
    "\t\timport json\n",
    "\t\twith open(labelmaps[-1], 'r', encoding='utf-8') as f:\n",
    "\t\t\tm = json.load(f)\n",
    "\t\t\tindex_to_label = {int(k): v for k, v in m.get('index_to_label', {}).items()}\n",
    "\t\tlabels = [index_to_label.get(int(i), str(i)) for i in pred_idx]\n",
    "\telse:\n",
    "\t\tlabels = pred_idx\n",
    "\tdf_out = df_new.copy()\n",
    "\tdf_out['pred_label'] = labels\n",
    "\n",
    "# Salvar (tolerante a arquivo aberto no Windows)\n",
    "Path(ARTIFACTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "output_path = Path(ARTIFACTS_DIR) / OUTPUT_PATH\n",
    "try:\n",
    "\tif output_path.suffix.lower() == '.csv':\n",
    "\t\tdf_out.to_csv(output_path, index=False)\n",
    "\telse:\n",
    "\t\tdf_out.to_excel(output_path, index=False, engine='openpyxl')\n",
    "except PermissionError:\n",
    "\timport time\n",
    "\talt_name = output_path.with_stem(output_path.stem + '_' + time.strftime('%Y%m%d-%H%M%S'))\n",
    "\tif alt_name.suffix.lower() == '.csv':\n",
    "\t\tdf_out.to_csv(alt_name, index=False)\n",
    "\telse:\n",
    "\t\tdf_out.to_excel(alt_name, index=False, engine='openpyxl')\n",
    "\toutput_path = alt_name\n",
    "\n",
    "print('Predições salvas em:', output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_24936\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições futuras salvas em: artifacts\\predicoes_futuras_2026-2030.xlsx\n",
      "Linhas geradas: 160\n"
     ]
    }
   ],
   "source": [
    "# Predições futuras para anos específicos (2026–2030) por tabela/ID\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "YEARS_TO_FORECAST = [2026, 2027, 2028, 2029, 2030]\n",
    "OUTPUT_FUTURE_FILE = 'predicoes_futuras_2026-2030.xlsx'\n",
    "\n",
    "assert 'df' in globals() and 'X' in globals(), 'Execute as células de carregamento e de engenharia antes.'\n",
    "assert 'preprocessor' in globals() and 'model' in globals(), 'Treine o modelo antes desta célula.'\n",
    "\n",
    "# Escolher melhor coluna de identificação disponível\n",
    "id_candidates = ['ID-ÚNICO-TT', 'ID-ÚNICO', 'ID-ÚNICO2', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'PLANILHA']\n",
    "id_col = None\n",
    "for c in id_candidates:\n",
    "\tif c in df.columns:\n",
    "\t\tid_col = c\n",
    "\t\tbreak\n",
    "\n",
    "# Base: última linha por ID (ou última do conjunto) para usar como \"perfil\" do projeto/tabela\n",
    "if id_col is not None:\n",
    "\tbase_df = df.dropna(subset=[id_col]).copy()\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\tbase_df = base_df.sort_values(by=main_date_col)\n",
    "\t\tbase_df = base_df.groupby(id_col, as_index=False).tail(1)\n",
    "\telse:\n",
    "\t\tbase_df = base_df.drop_duplicates(subset=[id_col], keep='last')\n",
    "else:\n",
    "\tbase_df = df.copy()\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\tbase_df = base_df.sort_values(by=main_date_col).tail(1)\n",
    "\n",
    "if len(base_df) == 0:\n",
    "\traise SystemExit('Não há linhas base para projetar futuras predições.')\n",
    "\n",
    "# Criar cenários para os anos desejados, ajustando campos de data/ano relevantes\n",
    "scenario_frames = []\n",
    "for year in YEARS_TO_FORECAST:\n",
    "\tf = base_df.copy()\n",
    "\t# Ajustes em colunas temporais comuns\n",
    "\tif 'ANO LEILÃO' in f.columns:\n",
    "\t\tf['ANO LEILÃO'] = int(year)\n",
    "\tif 'ANO DA CONCESSÃO' in f.columns:\n",
    "\t\tf['ANO DA CONCESSÃO'] = int(year)\n",
    "\tif 'DATA DE INÍCIO' in f.columns:\n",
    "\t\ttry:\n",
    "\t\t\tf['DATA DE INÍCIO'] = pd.Timestamp(int(year), 1, 1)\n",
    "\t\texcept Exception:\n",
    "\t\t\tf['DATA DE INÍCIO'] = pd.NaT\n",
    "\t# Se usamos uma coluna temporal principal, sincronizar features derivadas\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in f.columns:\n",
    "\t\ttry:\n",
    "\t\t\tf[main_date_col] = pd.to_datetime(pd.Series([pd.Timestamp(int(year), 1, 1)] * len(f)))\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\t# Rótulo do ano previsto\n",
    "\tf['ANO_PREVISTO'] = int(year)\n",
    "\tscenario_frames.append(f)\n",
    "\n",
    "df_future = pd.concat(scenario_frames, ignore_index=True)\n",
    "\n",
    "# Garantir que teremos exatamente as mesmas colunas de entrada do treino\n",
    "for col in [c for c in X.columns if c not in df_future.columns]:\n",
    "\tdf_future[col] = pd.NA\n",
    "\n",
    "# Transformar e prever\n",
    "X_future = df_future[X.columns]\n",
    "X_future_t = preprocessor.transform(X_future)\n",
    "proba_or_pred = model.predict(X_future_t, verbose=0)\n",
    "\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = proba_or_pred.reshape(-1)\n",
    "\tpred_col_name = 'pred_' + str(TARGET_COLUMN).replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')\n",
    "\tdf_future_out = df_future.copy()\n",
    "\tdf_future_out[pred_col_name] = y_pred\n",
    "else:\n",
    "\tpred_idx = proba_or_pred.argmax(axis=1)\n",
    "\t# Mapear rótulos se disponível\n",
    "\tlabels = pred_idx\n",
    "\ttry:\n",
    "\t\tfrom pathlib import Path as _P\n",
    "\t\tlabelmaps = sorted((_P(MODEL_DIR)).glob('labelmap_*.json'))\n",
    "\t\tif labelmaps:\n",
    "\t\t\timport json\n",
    "\t\t\twith open(labelmaps[-1], 'r', encoding='utf-8') as f:\n",
    "\t\t\t\tm = json.load(f)\n",
    "\t\t\t\tindex_to_label = {int(k): v for k, v in m.get('index_to_label', {}).items()}\n",
    "\t\t\tlabels = [index_to_label.get(int(i), str(i)) for i in pred_idx]\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\tdf_future_out = df_future.copy()\n",
    "\tdf_future_out['pred_label'] = labels\n",
    "\n",
    "# Selecionar colunas chave para saída, mantendo identificadores úteis\n",
    "id_out_cols = [c for c in ['ANO_PREVISTO', 'ID-ÚNICO-TT', 'ID-ÚNICO', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'UF', 'SETOR', 'PLANILHA'] if c in df_future_out.columns]\n",
    "value_cols = [c for c in df_future_out.columns if c.startswith('pred_') or c == 'pred_label']\n",
    "other_cols = []\n",
    "\n",
    "df_export = df_future_out[id_out_cols + value_cols + other_cols]\n",
    "\n",
    "# Salvar\n",
    "Path(ARTIFACTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "output_path = Path(ARTIFACTS_DIR) / OUTPUT_FUTURE_FILE\n",
    "if output_path.suffix.lower() == '.csv':\n",
    "\tdf_export.to_csv(output_path, index=False)\n",
    "else:\n",
    "\tdf_export.to_excel(output_path, index=False)\n",
    "\n",
    "print('Predições futuras salvas em:', output_path)\n",
    "print('Linhas geradas:', len(df_export))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
