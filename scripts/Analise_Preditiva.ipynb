{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise preditiva\n",
    "\n",
    "Siga estes passos:\n",
    "- Ajuste `TARGET_COLUMN` na próxima célula (ou deixe `None` para autodetecção).\n",
    "- Execute as células em ordem, de cima para baixo.\n",
    "- Os arquivos `.xlsx` serão lidos do diretório atual e da pasta `Dados/`.\n",
    "- Saídas: modelos em `models/` e métricas em `artifacts/`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dica: execute primeiro as células de configuração e de carregamento de dados. Depois use a célula de \"Preview de colunas\" mais abaixo.\n"
     ]
    }
   ],
   "source": [
    "# Ajuda rápida: visualizar colunas e candidatos a alvo\n",
    "print('Dica: execute primeiro as células de configuração e de carregamento de dados. Depois use a célula de \"Preview de colunas\" mais abaixo.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Versões:\n",
      "Python: 3.13.7 (tags/v3.13.7:bcee1c3, Aug 14 2025, 14:15:11) [MSC v.1944 64 bit (AMD64)]\n",
      "Pandas: 2.3.2\n",
      "Scikit-learn: 1.7.2\n",
      "TensorFlow: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "# Configuração inicial\n",
    "# Ajuste estes parâmetros conforme seu caso\n",
    "TARGET_COLUMN = 'INVESTIMENTO TOTAL (BI)'\n",
    "DATE_COLUMNS_CANDIDATES = ['data', 'dt', 'date', 'competencia', 'mes', 'mês', 'ano']\n",
    "ID_COLUMNS_CANDIDATES = ['id', 'codigo', 'código', 'cod', 'cód', 'cnpj', 'cpf']\n",
    "TASK_TYPE = 'regression'  # regressão sobre INVESTIMENTO TOTAL (BI)\n",
    "SHEET_NAME_PREFERRED = 'todas as tabelas'\n",
    "PREFERRED_COLUMNS = [\n",
    "\t'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'EXTENSÃO (km)', '(km)%', '(km)% EXEC', '(km)% PLAN',\n",
    "\t'Ext. (km)', 'FINANCEIRO (R$)', 'FINANCEIRO PLAN (R$)', 'PERCENTUAL (%)', 'PERCENTUAL (%) EXEC', 'PERCENTUAL (%) PLAN'\n",
    "]\n",
    "PRIORITY_TARGETS = ['INVESTIMENTO TOTAL (BI)', 'PERCENTUAL (%)', 'FINANCEIRO (R$)']\n",
    "TEST_SIZE = 0.1\n",
    "VAL_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "MAX_EPOCHS = 200\n",
    "BATCH_SIZE = 64\n",
    "PATIENCE = 20\n",
    "MODEL_DIR = 'models'\n",
    "ARTIFACTS_DIR = 'artifacts'\n",
    "# Limite opcional de linhas para acelerar (None desativa)\n",
    "MAX_ROWS = None\n",
    "\n",
    "\n",
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import json\n",
    "import glob\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Optional installs (executa apenas se faltar pacote)\n",
    "try:\n",
    "\timport sklearn  # type: ignore\n",
    "except Exception:\n",
    "\t!pip -q install scikit-learn\n",
    "\timport sklearn  # type: ignore\n",
    "\n",
    "try:\n",
    "\timport tensorflow as tf  # type: ignore\n",
    "except Exception:\n",
    "\t!pip -q install tensorflow==2.*\n",
    "\timport tensorflow as tf  # type: ignore\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, accuracy_score, f1_score, classification_report\n",
    "\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(ARTIFACTS_DIR, exist_ok=True)\n",
    "\n",
    "print('Versões:')\n",
    "print('Python:', sys.version)\n",
    "print('Pandas:', pd.__version__)\n",
    "print('Scikit-learn:', sklearn.__version__)\n",
    "print('TensorFlow:', tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carregue os dados primeiro.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if 'df_raw' in globals():\n",
    "\tif TARGET_COLUMN is None:\n",
    "\t\tfor t in PRIORITY_TARGETS:\n",
    "\t\t\tif t in df_raw.columns:\n",
    "\t\t\t\tTARGET_COLUMN = t\n",
    "\t\t\t\tprint('TARGET_COLUMN escolhido por prioridade =', TARGET_COLUMN)\n",
    "\t\t\t\tbreak\n",
    "\t\tif TARGET_COLUMN is None:\n",
    "\t\t\tprint('Nenhum alvo prioritário encontrado; manter heurística padrão.')\n",
    "else:\n",
    "\tprint('Carregue os dados primeiro.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 7 Excel files:\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 6.xlsx\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 7.xlsx\n",
      "- Planilha Monitoramento_PPI - Carregamento - BI 2025 - 8.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 1.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 2.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 3.xlsx\n",
      "- Dados\\Planilha Monitoramento_PPI - Carregamento - BI 2025 - 6.xlsx\n",
      "Raw shape: (606526, 49)\n",
      "Columns: ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', '__source_file', '__source_sheet', 'ID-ÚNICO', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'ID-ÚNICO2', 'SETOR2', 'EMPREENDIMENTO2']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Região</th>\n",
       "      <th>BR</th>\n",
       "      <th>EMPREENDIMENTO</th>\n",
       "      <th>PROPONENTE</th>\n",
       "      <th>EXECUTOR (Grupo Controlador)</th>\n",
       "      <th>ESTRUTURADOR DO PROJETO</th>\n",
       "      <th>ANO LEILÃO</th>\n",
       "      <th>DATA DE INÍCIO</th>\n",
       "      <th>ANO DA CONCESSÃO</th>\n",
       "      <th>PRAZO (anos)</th>\n",
       "      <th>...</th>\n",
       "      <th>(km)% PLAN</th>\n",
       "      <th>Descrição</th>\n",
       "      <th>Ext. (km)</th>\n",
       "      <th>FINANCEIRO (R$)</th>\n",
       "      <th>FINANCEIRO PLAN (R$)</th>\n",
       "      <th>PERCENTUAL (%)</th>\n",
       "      <th>PERCENTUAL (%) EXEC</th>\n",
       "      <th>PERCENTUAL (%) PLAN</th>\n",
       "      <th>km (f)</th>\n",
       "      <th>km (i)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SUL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VIA SUL</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>CCR</td>\n",
       "      <td>EPL</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>2019-02-15</td>\n",
       "      <td>7.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SD/CO</td>\n",
       "      <td>1.0</td>\n",
       "      <td>CERRADO</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>ECORODOVIAS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>6.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SUL</td>\n",
       "      <td>1.0</td>\n",
       "      <td>VIA COSTEIRA</td>\n",
       "      <td>ANTT</td>\n",
       "      <td>CCR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2020.0</td>\n",
       "      <td>2020-07-08</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 49 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Região   BR EMPREENDIMENTO PROPONENTE EXECUTOR (Grupo Controlador)  \\\n",
       "0    SUL  1.0        VIA SUL       ANTT                          CCR   \n",
       "1  SD/CO  1.0        CERRADO       ANTT                  ECORODOVIAS   \n",
       "2    SUL  1.0   VIA COSTEIRA       ANTT                          CCR   \n",
       "\n",
       "  ESTRUTURADOR DO PROJETO  ANO LEILÃO DATA DE INÍCIO  ANO DA CONCESSÃO  \\\n",
       "0                     EPL      2018.0     2019-02-15               7.0   \n",
       "1                     NaN      2019.0     2020-01-20               6.0   \n",
       "2                     NaN      2020.0     2020-07-08               5.0   \n",
       "\n",
       "   PRAZO (anos)  ...  (km)% PLAN  Descrição  Ext. (km) FINANCEIRO (R$)  \\\n",
       "0          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "1          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "2          30.0  ...         NaN        NaN        NaN             NaN   \n",
       "\n",
       "  FINANCEIRO PLAN (R$)  PERCENTUAL (%) PERCENTUAL (%) EXEC  \\\n",
       "0                  NaN             NaN                 NaN   \n",
       "1                  NaN             NaN                 NaN   \n",
       "2                  NaN             NaN                 NaN   \n",
       "\n",
       "  PERCENTUAL (%) PLAN km (f) km (i)  \n",
       "0                 NaN    NaN    NaN  \n",
       "1                 NaN    NaN    NaN  \n",
       "2                 NaN    NaN    NaN  \n",
       "\n",
       "[3 rows x 49 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from datetime import datetime\n",
    "\n",
    "ROOT = Path('.')\n",
    "DATA_DIRS = [ROOT, ROOT / 'Dados']\n",
    "excel_files = []\n",
    "for d in DATA_DIRS:\n",
    "\tif d.exists():\n",
    "\t\texcel_files.extend(sorted(map(Path, glob.glob(str(d / '*.xlsx')))))\n",
    "\n",
    "if not excel_files:\n",
    "\traise FileNotFoundError('No .xlsx files found in project root or Dados/.')\n",
    "\n",
    "print(f'Found {len(excel_files)} Excel files:')\n",
    "for p in excel_files:\n",
    "\tprint('-', p)\n",
    "\n",
    "frames = []\n",
    "for fp in excel_files:\n",
    "\ttry:\n",
    "\t\txl = pd.ExcelFile(fp)\n",
    "\t\tsheets = xl.sheet_names\n",
    "\t\tif SHEET_NAME_PREFERRED in sheets:\n",
    "\t\t\tcandidate_sheets = [SHEET_NAME_PREFERRED]\n",
    "\t\telse:\n",
    "\t\t\tcandidate_sheets = sheets\n",
    "\t\tfor sheet in candidate_sheets:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf = xl.parse(sheet)\n",
    "\t\t\t\tdf['__source_file'] = fp.name\n",
    "\t\t\t\tdf['__source_sheet'] = sheet\n",
    "\t\t\t\tframes.append(df)\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'Warning parsing sheet {sheet} in {fp.name}: {e}')\n",
    "\texcept Exception as e:\n",
    "\t\tprint(f'Warning opening {fp.name}: {e}')\n",
    "\n",
    "if not frames:\n",
    "\traise RuntimeError('Could not read any sheet from Excel files.')\n",
    "\n",
    "df_raw = pd.concat(frames, ignore_index=True)\n",
    "print('Raw shape:', df_raw.shape)\n",
    "print('Columns:', list(df_raw.columns)[:30])\n",
    "\n",
    "df_raw.head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colunas (primeiras 30): ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', '__source_file', '__source_sheet', 'ID-ÚNICO', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'ID-ÚNICO2', 'SETOR2', 'EMPREENDIMENTO2']\n",
      "Tipos:\n",
      "Região                                  object\n",
      "BR                                     float64\n",
      "EMPREENDIMENTO                          object\n",
      "PROPONENTE                              object\n",
      "EXECUTOR (Grupo Controlador)            object\n",
      "ESTRUTURADOR DO PROJETO                 object\n",
      "ANO LEILÃO                             float64\n",
      "DATA DE INÍCIO                  datetime64[ns]\n",
      "ANO DA CONCESSÃO                       float64\n",
      "PRAZO (anos)                           float64\n",
      "CAPEX (BI)                             float64\n",
      "OPEX (BI)                              float64\n",
      "INVESTIMENTO TOTAL (BI)                float64\n",
      "ETAPA                                   object\n",
      "SITUAÇÃO                                object\n",
      "EXTENSÃO (km)                          float64\n",
      "ESTADO/LOTE                             object\n",
      "PLANILHA                                object\n",
      "__source_file                           object\n",
      "__source_sheet                          object\n",
      "ID-ÚNICO                               float64\n",
      "SETOR                                   object\n",
      "UF                                      object\n",
      "Atributo.1                              object\n",
      "Atributo.2                              object\n",
      "Atributo.3                              object\n",
      "Valor                                   object\n",
      "ID-ÚNICO2                              float64\n",
      "SETOR2                                  object\n",
      "EMPREENDIMENTO2                         object\n",
      "dtype: object\n",
      "Possíveis alvos numéricos: ['BR', 'ANO LEILÃO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)', 'EXTENSÃO (km)', 'ID-ÚNICO', 'ID-ÚNICO2']\n",
      "Possíveis alvos categóricos: ['Região', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'DATA DE INÍCIO', 'ETAPA', 'SITUAÇÃO', 'ESTADO/LOTE', 'PLANILHA']\n"
     ]
    }
   ],
   "source": [
    "# Preview de colunas (execute após carregar df_raw)\n",
    "if 'df_raw' not in globals():\n",
    "\tprint('Carregue os dados primeiro.')\n",
    "else:\n",
    "\tprint('Colunas (primeiras 30):', list(df_raw.columns)[:30])\n",
    "\tprint('Tipos:')\n",
    "\tprint(df_raw.dtypes.head(30))\n",
    "\tnum_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "\tcat_cols = [c for c in df_raw.columns if c not in num_cols]\n",
    "\tprint('Possíveis alvos numéricos:', num_cols[:10])\n",
    "\tprint('Possíveis alvos categóricos:', cat_cols[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Correção BR aplicada onde possível nas colunas preferidas.\n"
     ]
    }
   ],
   "source": [
    "# Correção de formatos BR para colunas preferidas\n",
    "# Remove R$, pontos de milhar, troca vírgula por ponto e trata %\n",
    "\n",
    "def br_to_float(series: pd.Series) -> pd.Series:\n",
    "\tif pd.api.types.is_numeric_dtype(series):\n",
    "\t\treturn series\n",
    "\ts = series.astype(str).str.strip()\n",
    "\ts = s.str.replace(r'\\s', '', regex=True)\n",
    "\ts = s.str.replace('R$', '', regex=False)\n",
    "\ts = s.str.replace('.', '', regex=False)\n",
    "\ts = s.str.replace(',', '.', regex=False)\n",
    "\t# Percentuais: transformar \"10%\" em 0.10\n",
    "\tis_percent = s.str.contains('%', regex=False)\n",
    "\ts = s.str.replace('%', '', regex=False)\n",
    "\tout = pd.to_numeric(s, errors='coerce')\n",
    "\tout = out.where(~is_percent, out / 100.0)\n",
    "\treturn out\n",
    "\n",
    "if 'df_raw' in globals():\n",
    "\tfor col in PREFERRED_COLUMNS:\n",
    "\t\tif col in df_raw.columns:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tdf_raw[col] = br_to_float(df_raw[col])\n",
    "\t\t\texcept Exception as e:\n",
    "\t\t\t\tprint(f'Falha ao converter {col}: {e}')\n",
    "\tprint('Correção BR aplicada onde possível nas colunas preferidas.')\n",
    "else:\n",
    "\tprint('Carregue os dados primeiro.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Candidate date columns: ['ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)']\n",
      "Candidate id columns: ['ID-ÚNICO', 'ID-ÚNICO2', 'ID-ÚNICO-TT']\n"
     ]
    }
   ],
   "source": [
    "# Infer target, date and id columns helpers\n",
    "\n",
    "def normalize_name(name: str) -> str:\n",
    "\treturn str(name).strip().lower().replace('\\n',' ').replace('\\r',' ')\n",
    "\n",
    "norm_cols = {c: normalize_name(c) for c in df_raw.columns}\n",
    "\n",
    "# Try to infer target if not provided\n",
    "if TARGET_COLUMN is None:\n",
    "\t# Heuristic: last numeric column\n",
    "\tnumeric_cols = [c for c in df_raw.columns if pd.api.types.is_numeric_dtype(df_raw[c])]\n",
    "\tTARGET_COLUMN = numeric_cols[-1] if numeric_cols else df_raw.columns[-1]\n",
    "\tprint('Auto-selected TARGET_COLUMN =', TARGET_COLUMN)\n",
    "else:\n",
    "\tassert TARGET_COLUMN in df_raw.columns, f'TARGET_COLUMN {TARGET_COLUMN} not in data.'\n",
    "\n",
    "# Detect date-like columns\n",
    "candidate_date_cols = []\n",
    "for c, n in norm_cols.items():\n",
    "\tif any(tok in n for tok in DATE_COLUMNS_CANDIDATES):\n",
    "\t\tcandidate_date_cols.append(c)\n",
    "\n",
    "# Also include truly datetime types\n",
    "for c in df_raw.columns:\n",
    "\tif pd.api.types.is_datetime64_any_dtype(df_raw[c]):\n",
    "\t\tcandidate_date_cols.append(c)\n",
    "\n",
    "candidate_date_cols = list(dict.fromkeys(candidate_date_cols))  # unique\n",
    "print('Candidate date columns:', candidate_date_cols)\n",
    "\n",
    "# Detect id-like columns\n",
    "candidate_id_cols = []\n",
    "for c, n in norm_cols.items():\n",
    "\tif any(tok in n for tok in ID_COLUMNS_CANDIDATES):\n",
    "\t\tcandidate_id_cols.append(c)\n",
    "print('Candidate id columns:', candidate_id_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using time column: ANO LEILÃO\n",
      "Feature columns (first 30): ['Região', 'BR', 'EMPREENDIMENTO', 'PROPONENTE', 'EXECUTOR (Grupo Controlador)', 'ESTRUTURADOR DO PROJETO', 'ANO LEILÃO', 'DATA DE INÍCIO', 'ANO DA CONCESSÃO', 'PRAZO (anos)', 'CAPEX (BI)', 'OPEX (BI)', 'ETAPA', 'SITUAÇÃO', 'EXTENSÃO (km)', 'ESTADO/LOTE', 'PLANILHA', 'SETOR', 'UF', 'Atributo.1', 'Atributo.2', 'Atributo.3', 'Valor', 'SETOR2', 'EMPREENDIMENTO2', 'Descrição2', 'Ext.(km)2', 'FINANCEIRO PLAN (R$)2', 'FINANCEIRO(R$)2', '(km)% PLAN2']\n",
      "X shape: (406, 47) y shape: (406,)\n",
      "TASK_TYPE = regression\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\3808863425.py:18: UserWarning: The argument 'infer_datetime_format' is deprecated and will be removed in a future version. A strict version of it is now the default, see https://pandas.pydata.org/pdeps/0004-consistent-to-datetime-parsing.html. You can safely remove this argument.\n",
      "  df[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n"
     ]
    }
   ],
   "source": [
    "# Build feature table\n",
    "df = df_raw.copy()\n",
    "\n",
    "# Clean target FIRST: coerce to numeric and drop NaN/inf\n",
    "if TARGET_COLUMN in df.columns:\n",
    "\tdf[TARGET_COLUMN] = pd.to_numeric(df[TARGET_COLUMN], errors='coerce')\n",
    "\tmask_target = df[TARGET_COLUMN].apply(lambda v: pd.notna(v) and np.isfinite(v))\n",
    "\tdf = df[mask_target]\n",
    "\n",
    "# Stop early if no rows remain\n",
    "if len(df) == 0:\n",
    "\traise SystemExit('Sem linhas com alvo válido após limpeza do alvo.')\n",
    "\n",
    "# Parse first date column if available\n",
    "main_date_col = None\n",
    "for c in candidate_date_cols:\n",
    "\ttry:\n",
    "\t\tdf[c] = pd.to_datetime(df[c], errors='coerce', dayfirst=True, infer_datetime_format=True)\n",
    "\t\tif df[c].notna().sum() > 0 and main_date_col is None:\n",
    "\t\t\tmain_date_col = c\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\n",
    "# Optional row limit to speed up (apply after knowing main_date_col)\n",
    "if 'MAX_ROWS' in globals() and MAX_ROWS is not None:\n",
    "\tif main_date_col and main_date_col in df.columns:\n",
    "\t\tdf = df.sort_values(by=main_date_col).tail(int(MAX_ROWS))\n",
    "\telse:\n",
    "\t\tdf = df.sample(n=int(MAX_ROWS), random_state=RANDOM_STATE) if len(df) > MAX_ROWS else df\n",
    "\n",
    "if main_date_col:\n",
    "\tdf['__year'] = df[main_date_col].dt.year\n",
    "\tdf['__month'] = df[main_date_col].dt.month\n",
    "\tdf['__day'] = df[main_date_col].dt.day\n",
    "\tdf['__quarter'] = df[main_date_col].dt.quarter\n",
    "\tprint('Using time column:', main_date_col)\n",
    "else:\n",
    "\tprint('No usable time column found; will use random split.')\n",
    "\n",
    "# Drop purely identifier columns and technical columns\n",
    "cols_to_drop = set(candidate_id_cols + ['__source_file', '__source_sheet'])\n",
    "feature_cols = [c for c in df.columns if c not in cols_to_drop and c != TARGET_COLUMN]\n",
    "\n",
    "# Separate X, y\n",
    "X = df[feature_cols]\n",
    "y = df[TARGET_COLUMN]\n",
    "\n",
    "print('Feature columns (first 30):', feature_cols[:30])\n",
    "print('X shape:', X.shape, 'y shape:', y.shape)\n",
    "\n",
    "# Determine task type\n",
    "if TASK_TYPE is None:\n",
    "\tif pd.api.types.is_numeric_dtype(y):\n",
    "\t\tunique_vals = pd.Series(y).nunique(dropna=True)\n",
    "\t\tTASK_TYPE = 'regression' if unique_vals > 15 else 'classification'\n",
    "\telse:\n",
    "\t\tTASK_TYPE = 'classification'\n",
    "print('TASK_TYPE =', TASK_TYPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric features: 22 Categorical features: 25\n"
     ]
    }
   ],
   "source": [
    "# Preprocessing with ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "numeric_features = [c for c in X.columns if pd.api.types.is_numeric_dtype(X[c])]\n",
    "categorical_features = [c for c in X.columns if c not in numeric_features]\n",
    "\n",
    "# Ensure categorical columns are uniformly strings (preserve NaN)\n",
    "def to_str_preserve_nan(df_in):\n",
    "\treturn df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
    "\n",
    "numeric_pipeline = Pipeline(steps=[\n",
    "\t('imputer_median', SimpleImputer(strategy='median')),\n",
    "\t('imputer_zero', SimpleImputer(strategy='constant', fill_value=0)),\n",
    "\t('scaler', StandardScaler(with_mean=True, with_std=True)),\n",
    "])\n",
    "\n",
    "categorical_pipeline = Pipeline(steps=[\n",
    "\t('to_str', FunctionTransformer(to_str_preserve_nan)),\n",
    "\t('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "\t('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False, min_frequency=0.01)),\n",
    "])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "\ttransformers=[\n",
    "\t\t('num', numeric_pipeline, numeric_features),\n",
    "\t\t('cat', categorical_pipeline, categorical_features),\n",
    "\t],\n",
    "\tremainder='drop',\n",
    ")\n",
    "\n",
    "print('Numeric features:', len(numeric_features), 'Categorical features:', len(categorical_features))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (330, 47) Val: (36, 47) Test: (40, 47)\n",
      "Transformed dims: (330, 142) (36, 142) (40, 142)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Train/Validation/Test split (time-aware if possible)\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "if main_date_col:\n",
    "\t# Sort by time and split by index ratios\n",
    "\tdf_sorted = df.sort_values(by=main_date_col)\n",
    "\tX_sorted = df_sorted[feature_cols]\n",
    "\ty_sorted = df_sorted[TARGET_COLUMN]\n",
    "\t\n",
    "\tn_total = len(df_sorted)\n",
    "\tn_test = max(1, int(math.floor(TEST_SIZE * n_total))) if n_total >= 3 else 1\n",
    "\tn_val_pool = max(0, n_total - n_test)\n",
    "\tn_val = max(1, int(math.floor(VAL_SIZE * n_val_pool))) if n_val_pool >= 3 else (1 if n_val_pool >= 2 else 0)\n",
    "\ttrain_end = n_total - n_test - n_val\n",
    "\tif train_end <= 0:\n",
    "\t\t# Fallback to random split to avoid empty sets\n",
    "\t\tX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=min(0.2, max(0.1, TEST_SIZE)), random_state=RANDOM_STATE, stratify=(y if TASK_TYPE=='classification' else None))\n",
    "\t\tX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=min(0.2, max(0.1, VAL_SIZE)), random_state=RANDOM_STATE, stratify=(y_temp if TASK_TYPE=='classification' else None))\n",
    "\telse:\n",
    "\t\tX_train = X_sorted.iloc[: train_end]\n",
    "\t\ty_train = y_sorted.iloc[: train_end]\n",
    "\t\tX_val = X_sorted.iloc[train_end : train_end + n_val]\n",
    "\t\ty_val = y_sorted.iloc[train_end : train_end + n_val]\n",
    "\t\tX_test = X_sorted.iloc[n_total - n_test :]\n",
    "\t\ty_test = y_sorted.iloc[n_total - n_test :]\n",
    "else:\n",
    "\tX_temp, X_test, y_temp, y_test = train_test_split(X, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=(y if TASK_TYPE=='classification' else None))\n",
    "\tX_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=VAL_SIZE, random_state=RANDOM_STATE, stratify=(y_temp if TASK_TYPE=='classification' else None))\n",
    "\n",
    "print('Train:', X_train.shape, 'Val:', X_val.shape, 'Test:', X_test.shape)\n",
    "\n",
    "# Guard against empty splits\n",
    "assert len(X_train) > 0 and len(X_val) > 0 and len(X_test) > 0, 'One of the splits is empty after splitting.'\n",
    "\n",
    "# Fit preprocessor on train only\n",
    "X_train_t = preprocessor.fit_transform(X_train)\n",
    "X_val_t = preprocessor.transform(X_val)\n",
    "X_test_t = preprocessor.transform(X_test)\n",
    "\n",
    "input_dim = X_train_t.shape[1]\n",
    "print('Transformed dims:', X_train_t.shape, X_val_t.shape, X_test_t.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 57ms/step - loss: 10.5523 - mae: 10.5523 - mse: 161.1356 - val_loss: 9.9809 - val_mae: 9.9809 - val_mse: 107.6275\n",
      "Epoch 2/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 10.5201 - mae: 10.5201 - mse: 157.7140 - val_loss: 9.8385 - val_mae: 9.8385 - val_mse: 104.5370\n",
      "Epoch 3/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 10.4096 - mae: 10.4096 - mse: 151.5777 - val_loss: 9.7211 - val_mae: 9.7211 - val_mse: 102.0679\n",
      "Epoch 4/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 10.2648 - mae: 10.2648 - mse: 148.0148 - val_loss: 9.5967 - val_mae: 9.5967 - val_mse: 99.5287\n",
      "Epoch 5/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 10.1979 - mae: 10.1979 - mse: 147.5347 - val_loss: 9.4881 - val_mae: 9.4881 - val_mse: 97.3751\n",
      "Epoch 6/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.9044 - mae: 9.9044 - mse: 142.3804 - val_loss: 9.3883 - val_mae: 9.3883 - val_mse: 95.3958\n",
      "Epoch 7/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 9.9052 - mae: 9.9052 - mse: 142.8525 - val_loss: 9.2690 - val_mae: 9.2690 - val_mse: 93.0407\n",
      "Epoch 8/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.7060 - mae: 9.7060 - mse: 137.1859 - val_loss: 9.1340 - val_mae: 9.1340 - val_mse: 90.4826\n",
      "Epoch 9/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.5850 - mae: 9.5850 - mse: 135.2772 - val_loss: 8.9932 - val_mae: 8.9932 - val_mse: 87.8706\n",
      "Epoch 10/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 9.5107 - mae: 9.5107 - mse: 131.6460 - val_loss: 8.8592 - val_mae: 8.8592 - val_mse: 85.3717\n",
      "Epoch 11/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 9.2940 - mae: 9.2940 - mse: 128.6025 - val_loss: 8.7198 - val_mae: 8.7198 - val_mse: 82.7541\n",
      "Epoch 12/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 9.0853 - mae: 9.0853 - mse: 122.4950 - val_loss: 8.5648 - val_mae: 8.5648 - val_mse: 79.9293\n",
      "Epoch 13/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 8.9387 - mae: 8.9387 - mse: 118.3769 - val_loss: 8.3957 - val_mae: 8.3957 - val_mse: 77.0551\n",
      "Epoch 14/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.7038 - mae: 8.7038 - mse: 114.0506 - val_loss: 8.1876 - val_mae: 8.1876 - val_mse: 73.5828\n",
      "Epoch 15/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 8.4299 - mae: 8.4299 - mse: 107.3623 - val_loss: 7.9235 - val_mae: 7.9235 - val_mse: 69.1518\n",
      "Epoch 16/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 8.0781 - mae: 8.0781 - mse: 98.0689 - val_loss: 7.6290 - val_mae: 7.6290 - val_mse: 64.2770\n",
      "Epoch 17/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 7.8848 - mae: 7.8848 - mse: 94.3602 - val_loss: 7.3211 - val_mae: 7.3211 - val_mse: 59.5810\n",
      "Epoch 18/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 7.6180 - mae: 7.6180 - mse: 88.5094 - val_loss: 7.0028 - val_mae: 7.0028 - val_mse: 54.7918\n",
      "Epoch 19/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 7.2506 - mae: 7.2506 - mse: 79.1860 - val_loss: 6.6938 - val_mae: 6.6938 - val_mse: 50.0296\n",
      "Epoch 20/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.9236 - mae: 6.9236 - mse: 71.3035 - val_loss: 6.3578 - val_mae: 6.3578 - val_mse: 45.1066\n",
      "Epoch 21/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.6211 - mae: 6.6211 - mse: 64.0340 - val_loss: 5.9530 - val_mae: 5.9530 - val_mse: 39.3597\n",
      "Epoch 22/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 6.1316 - mae: 6.1316 - mse: 53.5478 - val_loss: 5.4906 - val_mae: 5.4906 - val_mse: 33.4786\n",
      "Epoch 23/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 5.6596 - mae: 5.6596 - mse: 47.1094 - val_loss: 5.1653 - val_mae: 5.1653 - val_mse: 29.7626\n",
      "Epoch 24/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 5.4199 - mae: 5.4199 - mse: 42.8612 - val_loss: 5.2189 - val_mae: 5.2189 - val_mse: 31.2107\n",
      "Epoch 25/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 4.8128 - mae: 4.8128 - mse: 34.4172 - val_loss: 4.9351 - val_mae: 4.9351 - val_mse: 28.5546\n",
      "Epoch 26/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 4.4783 - mae: 4.4783 - mse: 31.8020 - val_loss: 4.4545 - val_mae: 4.4545 - val_mse: 23.6544\n",
      "Epoch 27/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step - loss: 3.7491 - mae: 3.7491 - mse: 20.3734 - val_loss: 3.7887 - val_mae: 3.7887 - val_mse: 17.1343\n",
      "Epoch 28/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 3.4535 - mae: 3.4535 - mse: 16.3349 - val_loss: 2.8991 - val_mae: 2.8991 - val_mse: 9.9581\n",
      "Epoch 29/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 2.9659 - mae: 2.9659 - mse: 13.4765 - val_loss: 2.4184 - val_mae: 2.4184 - val_mse: 6.8773\n",
      "Epoch 30/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.7401 - mae: 2.7401 - mse: 11.9173 - val_loss: 2.1444 - val_mae: 2.1444 - val_mse: 5.8274\n",
      "Epoch 31/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.3594 - mae: 2.3594 - mse: 9.0175 - val_loss: 1.3904 - val_mae: 1.3904 - val_mse: 2.5569\n",
      "Epoch 32/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 2.1129 - mae: 2.1129 - mse: 7.4479 - val_loss: 1.2037 - val_mae: 1.2037 - val_mse: 1.8175\n",
      "Epoch 33/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.2100 - mae: 2.2100 - mse: 7.6020 - val_loss: 0.9658 - val_mae: 0.9658 - val_mse: 1.8874\n",
      "Epoch 34/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 2.1051 - mae: 2.1051 - mse: 7.5894 - val_loss: 0.9377 - val_mae: 0.9377 - val_mse: 2.1790\n",
      "Epoch 35/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.8363 - mae: 1.8363 - mse: 5.6866 - val_loss: 1.0397 - val_mae: 1.0397 - val_mse: 1.9860\n",
      "Epoch 36/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step - loss: 2.1030 - mae: 2.1030 - mse: 7.1485 - val_loss: 0.9903 - val_mae: 0.9903 - val_mse: 2.1222\n",
      "Epoch 37/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step - loss: 1.9814 - mae: 1.9814 - mse: 6.4214 - val_loss: 0.8496 - val_mae: 0.8496 - val_mse: 2.2618\n",
      "Epoch 38/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step - loss: 1.9686 - mae: 1.9686 - mse: 6.3260 - val_loss: 0.7923 - val_mae: 0.7923 - val_mse: 1.9184\n",
      "Epoch 39/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step - loss: 1.7121 - mae: 1.7121 - mse: 4.8321 - val_loss: 0.7942 - val_mae: 0.7942 - val_mse: 1.6890\n",
      "Epoch 40/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step - loss: 2.2256 - mae: 2.2256 - mse: 7.8188 - val_loss: 0.7492 - val_mae: 0.7492 - val_mse: 1.3967\n",
      "Epoch 41/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.7776 - mae: 1.7776 - mse: 5.1418 - val_loss: 0.8149 - val_mae: 0.8149 - val_mse: 1.1381\n",
      "Epoch 42/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step - loss: 1.9397 - mae: 1.9397 - mse: 6.3559 - val_loss: 0.7809 - val_mae: 0.7809 - val_mse: 1.2227\n",
      "Epoch 43/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step - loss: 1.6005 - mae: 1.6005 - mse: 4.1282 - val_loss: 0.9371 - val_mae: 0.9371 - val_mse: 2.1626\n",
      "Epoch 44/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step - loss: 1.8690 - mae: 1.8690 - mse: 5.8651 - val_loss: 0.9954 - val_mae: 0.9954 - val_mse: 1.7904\n",
      "Epoch 45/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.6535 - mae: 1.6535 - mse: 4.4288 - val_loss: 1.0511 - val_mae: 1.0511 - val_mse: 1.4182\n",
      "Epoch 46/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.8995 - mae: 1.8995 - mse: 6.0869 - val_loss: 0.7509 - val_mae: 0.7509 - val_mse: 0.8761\n",
      "Epoch 47/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6295 - mae: 1.6295 - mse: 4.2740 - val_loss: 0.9411 - val_mae: 0.9411 - val_mse: 1.6712\n",
      "Epoch 48/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.8417 - mae: 1.8417 - mse: 6.2656 - val_loss: 0.8218 - val_mae: 0.8218 - val_mse: 1.2207\n",
      "Epoch 49/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7254 - mae: 1.7254 - mse: 5.1358 - val_loss: 0.6841 - val_mae: 0.6841 - val_mse: 1.0942\n",
      "Epoch 50/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.9932 - mae: 1.9932 - mse: 6.2743 - val_loss: 0.6860 - val_mae: 0.6860 - val_mse: 1.0617\n",
      "Epoch 51/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6375 - mae: 1.6375 - mse: 4.6477 - val_loss: 0.8316 - val_mae: 0.8316 - val_mse: 1.5530\n",
      "Epoch 52/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.7558 - mae: 1.7558 - mse: 5.0003 - val_loss: 0.9362 - val_mae: 0.9362 - val_mse: 1.5955\n",
      "Epoch 53/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.9113 - mae: 1.9113 - mse: 5.8356 - val_loss: 0.7282 - val_mae: 0.7282 - val_mse: 0.7245\n",
      "Epoch 54/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step - loss: 1.5593 - mae: 1.5593 - mse: 4.3628 - val_loss: 0.7525 - val_mae: 0.7525 - val_mse: 0.7480\n",
      "Epoch 55/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.7794 - mae: 1.7794 - mse: 5.3243 - val_loss: 0.6761 - val_mae: 0.6761 - val_mse: 0.6692\n",
      "Epoch 56/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step - loss: 1.6040 - mae: 1.6040 - mse: 4.6504 - val_loss: 0.9997 - val_mae: 0.9997 - val_mse: 1.2076\n",
      "Epoch 57/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4245 - mae: 1.4245 - mse: 3.3980 - val_loss: 1.1134 - val_mae: 1.1134 - val_mse: 1.4841\n",
      "Epoch 58/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6133 - mae: 1.6133 - mse: 4.3157 - val_loss: 0.7785 - val_mae: 0.7785 - val_mse: 0.7518\n",
      "Epoch 59/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.5425 - mae: 1.5425 - mse: 4.3035 - val_loss: 0.7503 - val_mae: 0.7503 - val_mse: 0.7151\n",
      "Epoch 60/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.6155 - mae: 1.6155 - mse: 4.2075 - val_loss: 0.7776 - val_mae: 0.7776 - val_mse: 0.7882\n",
      "Epoch 61/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.7113 - mae: 1.7113 - mse: 5.1594 - val_loss: 0.5844 - val_mae: 0.5844 - val_mse: 0.5125\n",
      "Epoch 62/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step - loss: 1.4749 - mae: 1.4749 - mse: 3.9025 - val_loss: 0.8752 - val_mae: 0.8752 - val_mse: 0.8442\n",
      "Epoch 63/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step - loss: 1.5448 - mae: 1.5448 - mse: 4.2228 - val_loss: 0.7573 - val_mae: 0.7573 - val_mse: 0.6764\n",
      "Epoch 64/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step - loss: 1.6194 - mae: 1.6194 - mse: 4.3505 - val_loss: 0.6352 - val_mae: 0.6352 - val_mse: 0.5573\n",
      "Epoch 65/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step - loss: 1.4184 - mae: 1.4184 - mse: 3.4239 - val_loss: 0.4649 - val_mae: 0.4649 - val_mse: 0.4097\n",
      "Epoch 66/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5925 - mae: 1.5925 - mse: 4.2703 - val_loss: 0.7792 - val_mae: 0.7792 - val_mse: 0.8476\n",
      "Epoch 67/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5061 - mae: 1.5061 - mse: 3.6412 - val_loss: 0.9133 - val_mae: 0.9133 - val_mse: 0.9522\n",
      "Epoch 68/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4093 - mae: 1.4093 - mse: 3.2629 - val_loss: 0.7928 - val_mae: 0.7928 - val_mse: 0.7499\n",
      "Epoch 69/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5868 - mae: 1.5868 - mse: 4.0630 - val_loss: 0.8923 - val_mae: 0.8923 - val_mse: 0.9602\n",
      "Epoch 70/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3537 - mae: 1.3537 - mse: 3.1460 - val_loss: 0.6937 - val_mae: 0.6937 - val_mse: 0.6084\n",
      "Epoch 71/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3358 - mae: 1.3358 - mse: 2.8156 - val_loss: 0.6504 - val_mae: 0.6504 - val_mse: 0.5348\n",
      "Epoch 72/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.4287 - mae: 1.4287 - mse: 3.2561 - val_loss: 0.7987 - val_mae: 0.7987 - val_mse: 0.7914\n",
      "Epoch 73/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6744 - mae: 1.6744 - mse: 4.7062 - val_loss: 0.7384 - val_mae: 0.7384 - val_mse: 0.7547\n",
      "Epoch 74/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6731 - mae: 1.6731 - mse: 4.6025 - val_loss: 0.6559 - val_mae: 0.6559 - val_mse: 0.6325\n",
      "Epoch 75/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.7093 - mae: 1.7093 - mse: 4.5457 - val_loss: 0.8260 - val_mae: 0.8260 - val_mse: 0.8557\n",
      "Epoch 76/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6580 - mae: 1.6580 - mse: 4.9706 - val_loss: 1.0188 - val_mae: 1.0188 - val_mse: 1.2150\n",
      "Epoch 77/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4866 - mae: 1.4866 - mse: 4.0799 - val_loss: 1.0312 - val_mae: 1.0312 - val_mse: 1.1677\n",
      "Epoch 78/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3257 - mae: 1.3257 - mse: 3.0955 - val_loss: 0.9367 - val_mae: 0.9367 - val_mse: 1.0356\n",
      "Epoch 79/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.3836 - mae: 1.3836 - mse: 3.4576 - val_loss: 0.9801 - val_mae: 0.9801 - val_mse: 1.0612\n",
      "Epoch 80/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6904 - mae: 1.6904 - mse: 4.6022 - val_loss: 1.0266 - val_mae: 1.0266 - val_mse: 1.2265\n",
      "Epoch 81/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.3928 - mae: 1.3928 - mse: 3.2376 - val_loss: 1.0035 - val_mae: 1.0035 - val_mse: 1.1573\n",
      "Epoch 82/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.5263 - mae: 1.5263 - mse: 4.1436 - val_loss: 0.7294 - val_mae: 0.7294 - val_mse: 0.6305\n",
      "Epoch 83/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step - loss: 1.4405 - mae: 1.4405 - mse: 3.7512 - val_loss: 0.8655 - val_mae: 0.8655 - val_mse: 0.8932\n",
      "Epoch 84/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.6227 - mae: 1.6227 - mse: 4.4591 - val_loss: 0.7787 - val_mae: 0.7787 - val_mse: 0.8200\n",
      "Epoch 85/200\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step - loss: 1.4763 - mae: 1.4763 - mse: 3.4604 - val_loss: 0.7423 - val_mae: 0.7423 - val_mse: 0.7674\n",
      "Best val metrics: 0.4649205207824707\n"
     ]
    }
   ],
   "source": [
    "# Define and train Keras model\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "\n",
    "def build_mlp(input_dim: int, task: str) -> keras.Model:\n",
    "\tunits = max(16, min(512, int(2 * math.sqrt(input_dim + 1)) * 16))\n",
    "\tinputs = keras.Input(shape=(input_dim,))\n",
    "\tx = layers.Dense(units, activation='relu')(inputs)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\tx = layers.Dropout(0.2)(x)\n",
    "\tx = layers.Dense(units // 2, activation='relu')(x)\n",
    "\tx = layers.BatchNormalization()(x)\n",
    "\tx = layers.Dropout(0.2)(x)\n",
    "\tif task == 'regression':\n",
    "\t\toutputs = layers.Dense(1, activation='linear')(x)\n",
    "\t\tloss = 'mae'\n",
    "\t\tmetrics = ['mae', 'mse']\n",
    "\telse:\n",
    "\t\t# detect classes\n",
    "\t\tif pd.api.types.is_numeric_dtype(y_train):\n",
    "\t\t\tclasses = np.unique(y_train.dropna())\n",
    "\t\t\tnum_classes = len(classes)\n",
    "\t\telse:\n",
    "\t\t\tclasses = np.unique(pd.Series(y_train).astype(str))\n",
    "\t\t\tnum_classes = len(classes)\n",
    "\t\toutputs = layers.Dense(num_classes, activation='softmax')(x)\n",
    "\t\tloss = 'sparse_categorical_crossentropy'\n",
    "\t\tmetrics = ['accuracy']\n",
    "\tmodel = keras.Model(inputs, outputs)\n",
    "\tmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=1e-3), loss=loss, metrics=metrics)\n",
    "\treturn model\n",
    "\n",
    "model = build_mlp(input_dim, TASK_TYPE)\n",
    "\n",
    "callbacks = [\n",
    "\tkeras.callbacks.EarlyStopping(monitor='val_loss', patience=PATIENCE, restore_best_weights=True),\n",
    "]\n",
    "\n",
    "# Prepare targets\n",
    "if TASK_TYPE == 'classification':\n",
    "\t# Map labels to integers for sparse_categorical_crossentropy\n",
    "\tlabel_series = pd.Series(y_train)\n",
    "\tlabel_to_index = {label: idx for idx, label in enumerate(sorted(label_series.dropna().unique(), key=lambda x: str(x)))}\n",
    "\tindex_to_label = {v: k for k, v in label_to_index.items()}\n",
    "\n",
    "y_train_arr = y_train.map(label_to_index).values if TASK_TYPE=='classification' else y_train.values\n",
    "\n",
    "y_val_arr = y_val.map(label_to_index).values if TASK_TYPE=='classification' else y_val.values\n",
    "\n",
    "y_test_arr = y_test.map(label_to_index).values if TASK_TYPE=='classification' else y_test.values\n",
    "\n",
    "history = model.fit(\n",
    "\tX_train_t, y_train_arr,\n",
    "\tvalidation_data=(X_val_t, y_val_arr),\n",
    "\tepochs=MAX_EPOCHS,\n",
    "\tbatch_size=BATCH_SIZE,\n",
    "\tverbose=1,\n",
    "\tcallbacks=callbacks,\n",
    ")\n",
    "\n",
    "print('Best val metrics:', min(history.history['val_loss']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test metrics: {'mae': 0.7327067518234249, 'r2': 0.9167322841013787}\n",
      "Saved:\n",
      "- models\\model_regression_20250923-115931.keras\n",
      "- models\\preprocessor_20250923-115931.joblib\n",
      "- artifacts\\metrics_regression_20250923-115931.json\n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save artifacts\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "# Predictions with NaN-safe evaluation\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = model.predict(X_test_t, verbose=0).reshape(-1)\n",
    "\ty_true_np = y_test.to_numpy()\n",
    "\tmask = (~np.isnan(y_true_np)) & (~np.isnan(y_pred))\n",
    "\tif mask.sum() == 0:\n",
    "\t\traise ValueError('No valid test rows to evaluate after NaN filtering.')\n",
    "\tmae_val = float(mean_absolute_error(y_true_np[mask], y_pred[mask]))\n",
    "\tr2_val = float(r2_score(y_true_np[mask], y_pred[mask]))\n",
    "\tmetrics_out = {'mae': mae_val, 'r2': r2_val}\n",
    "else:\n",
    "\tproba = model.predict(X_test_t, verbose=0)\n",
    "\ty_pred_idx = proba.argmax(axis=1)\n",
    "\tmask = ~pd.isna(y_test)\n",
    "\tif mask.sum() == 0:\n",
    "\t\traise ValueError('No valid test rows to evaluate after NaN filtering (classification).')\n",
    "\ty_true = y_test[mask]\n",
    "\ty_pred = pd.Series(y_pred_idx)[mask.to_numpy()].map(index_to_label)\n",
    "\tmetrics_out = {\n",
    "\t\t'accuracy': float(accuracy_score(y_true, y_pred)),\n",
    "\t\t'f1_macro': float(f1_score(y_true, y_pred, average='macro')),\n",
    "\t}\n",
    "\tprint(classification_report(y_true, y_pred))\n",
    "\n",
    "print('Test metrics:', metrics_out)\n",
    "\n",
    "# Save artifacts\n",
    "run_id = time.strftime('%Y%m%d-%H%M%S')\n",
    "model_path = Path(MODEL_DIR) / f'model_{TASK_TYPE}_{run_id}.keras'\n",
    "preproc_path = Path(MODEL_DIR) / f'preprocessor_{run_id}.joblib'\n",
    "report_path = Path(ARTIFACTS_DIR) / f'metrics_{TASK_TYPE}_{run_id}.json'\n",
    "labelmap_path = Path(MODEL_DIR) / f'labelmap_{run_id}.json'\n",
    "\n",
    "model.save(model_path)\n",
    "joblib.dump(preprocessor, preproc_path)\n",
    "\n",
    "with open(report_path, 'w', encoding='utf-8') as f:\n",
    "\tjson.dump(metrics_out, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "if TASK_TYPE == 'classification':\n",
    "\twith open(labelmap_path, 'w', encoding='utf-8') as f:\n",
    "\t\tjson.dump({'label_to_index': {str(k): int(v) for k, v in label_to_index.items()}, 'index_to_label': {str(k): str(v) for k, v in index_to_label.items()}}, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print('Saved:')\n",
    "print('-', model_path)\n",
    "print('-', preproc_path)\n",
    "print('-', report_path)\n",
    "if TASK_TYPE == 'classification':\n",
    "\tprint('-', labelmap_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction output shape: (1, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Example: inference on new data row\n",
    "# Provide a single-row DataFrame with same columns as X\n",
    "example = X.head(1).copy()\n",
    "X_new_t = preprocessor.transform(example)\n",
    "proba_or_pred = model.predict(X_new_t, verbose=0)\n",
    "print('Prediction output shape:', proba_or_pred.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usando modelo: models\\model_regression_20250923-115931.keras\n",
      "Usando preprocessor: models\\preprocessor_20250923-115931.joblib\n",
      "INPUT_PATH não definido; usando amostra de X.head(100).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições salvas em: artifacts\\predicoes.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Inference: carregar dados novos e salvar predicoes.xlsx\n",
    "import argparse\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from tensorflow import keras\n",
    "from pathlib import Path\n",
    "\n",
    "# Parâmetros\n",
    "INPUT_PATH = None  # ex.: 'novos_dados.xlsx' ou 'novos_dados.csv'; se None, usa X.head(100)\n",
    "OUTPUT_PATH = 'predicoes.xlsx'\n",
    "\n",
    "# Localizar últimos artifacts gerados\n",
    "models_dir = Path(MODEL_DIR)\n",
    "artifacts_dir = Path(ARTIFACTS_DIR)\n",
    "latest_model = sorted(models_dir.glob('model_regression_*.keras') if TASK_TYPE=='regression' else models_dir.glob('model_classification_*.keras'))[-1]\n",
    "latest_preproc = sorted(models_dir.glob('preprocessor_*.joblib'))[-1]\n",
    "\n",
    "print('Usando modelo:', latest_model)\n",
    "print('Usando preprocessor:', latest_preproc)\n",
    "\n",
    "preprocessor = joblib.load(latest_preproc)\n",
    "model = keras.models.load_model(latest_model)\n",
    "\n",
    "# Carregar dados novos\n",
    "if INPUT_PATH is None:\n",
    "\tprint('INPUT_PATH não definido; usando amostra de X.head(100).')\n",
    "\tdf_new = X.head(100).copy()\n",
    "else:\n",
    "\tp = Path(INPUT_PATH)\n",
    "\tif p.suffix.lower() in ['.xlsx', '.xls']:\n",
    "\t\tdf_new = pd.read_excel(p)\n",
    "\telif p.suffix.lower() == '.csv':\n",
    "\t\tdf_new = pd.read_csv(p)\n",
    "\telse:\n",
    "\t\traise ValueError('Formato não suportado: use .xlsx, .xls ou .csv')\n",
    "\n",
    "# Garantir mesmas colunas de treino (faltantes serão criadas vazias; extras serão ignoradas pelo preprocessor)\n",
    "for c in [col for col in X.columns if col not in df_new.columns]:\n",
    "\tdf_new[c] = pd.NA\n",
    "\n",
    "X_new_t = preprocessor.transform(df_new[X.columns])\n",
    "proba_or_pred = model.predict(X_new_t, verbose=0)\n",
    "\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = proba_or_pred.reshape(-1)\n",
    "\tdf_out = df_new.copy()\n",
    "\tdf_out['pred_'+str(TARGET_COLUMN).replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')] = y_pred\n",
    "else:\n",
    "\tpred_idx = proba_or_pred.argmax(axis=1)\n",
    "\t# Recarregar labelmap mais recente, se existir\n",
    "\tlabelmaps = sorted(models_dir.glob('labelmap_*.json'))\n",
    "\tif labelmaps:\n",
    "\t\timport json\n",
    "\t\twith open(labelmaps[-1], 'r', encoding='utf-8') as f:\n",
    "\t\t\tm = json.load(f)\n",
    "\t\t\tindex_to_label = {int(k): v for k, v in m.get('index_to_label', {}).items()}\n",
    "\t\tlabels = [index_to_label.get(int(i), str(i)) for i in pred_idx]\n",
    "\telse:\n",
    "\t\tlabels = pred_idx\n",
    "\tdf_out = df_new.copy()\n",
    "\tdf_out['pred_label'] = labels\n",
    "\n",
    "# Salvar (tolerante a arquivo aberto no Windows)\n",
    "Path(ARTIFACTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "output_path = Path(ARTIFACTS_DIR) / OUTPUT_PATH\n",
    "try:\n",
    "\tif output_path.suffix.lower() == '.csv':\n",
    "\t\tdf_out.to_csv(output_path, index=False)\n",
    "\telse:\n",
    "\t\tdf_out.to_excel(output_path, index=False, engine='openpyxl')\n",
    "except PermissionError:\n",
    "\timport time\n",
    "\talt_name = output_path.with_stem(output_path.stem + '_' + time.strftime('%Y%m%d-%H%M%S'))\n",
    "\tif alt_name.suffix.lower() == '.csv':\n",
    "\t\tdf_out.to_csv(alt_name, index=False)\n",
    "\telse:\n",
    "\t\tdf_out.to_excel(alt_name, index=False, engine='openpyxl')\n",
    "\toutput_path = alt_name\n",
    "\n",
    "print('Predições salvas em:', output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predições futuras salvas em: artifacts\\predicoes_futuras_2026-2030.xlsx\n",
      "Linhas geradas: 160\n"
     ]
    }
   ],
   "source": [
    "# Predições futuras para anos específicos (2026–2030) por tabela/ID\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "YEARS_TO_FORECAST = [2026, 2027, 2028, 2029, 2030]\n",
    "OUTPUT_FUTURE_FILE = 'predicoes_futuras_2026-2030.xlsx'\n",
    "\n",
    "assert 'df' in globals() and 'X' in globals(), 'Execute as células de carregamento e de engenharia antes.'\n",
    "assert 'preprocessor' in globals() and 'model' in globals(), 'Treine o modelo antes desta célula.'\n",
    "\n",
    "# Escolher melhor coluna de identificação disponível\n",
    "id_candidates = ['ID-ÚNICO-TT', 'ID-ÚNICO', 'ID-ÚNICO2', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'PLANILHA']\n",
    "id_col = None\n",
    "for c in id_candidates:\n",
    "\tif c in df.columns:\n",
    "\t\tid_col = c\n",
    "\t\tbreak\n",
    "\n",
    "# Base: última linha por ID (ou última do conjunto) para usar como \"perfil\" do projeto/tabela\n",
    "if id_col is not None:\n",
    "\tbase_df = df.dropna(subset=[id_col]).copy()\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\tbase_df = base_df.sort_values(by=main_date_col)\n",
    "\t\tbase_df = base_df.groupby(id_col, as_index=False).tail(1)\n",
    "\telse:\n",
    "\t\tbase_df = base_df.drop_duplicates(subset=[id_col], keep='last')\n",
    "else:\n",
    "\tbase_df = df.copy()\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\tbase_df = base_df.sort_values(by=main_date_col).tail(1)\n",
    "\n",
    "if len(base_df) == 0:\n",
    "\traise SystemExit('Não há linhas base para projetar futuras predições.')\n",
    "\n",
    "# Criar cenários para os anos desejados, ajustando campos de data/ano relevantes\n",
    "scenario_frames = []\n",
    "for year in YEARS_TO_FORECAST:\n",
    "\tf = base_df.copy()\n",
    "\t# Ajustes em colunas temporais comuns\n",
    "\tif 'ANO LEILÃO' in f.columns:\n",
    "\t\tf['ANO LEILÃO'] = int(year)\n",
    "\tif 'ANO DA CONCESSÃO' in f.columns:\n",
    "\t\tf['ANO DA CONCESSÃO'] = int(year)\n",
    "\tif 'DATA DE INÍCIO' in f.columns:\n",
    "\t\ttry:\n",
    "\t\t\tf['DATA DE INÍCIO'] = pd.Timestamp(int(year), 1, 1)\n",
    "\t\texcept Exception:\n",
    "\t\t\tf['DATA DE INÍCIO'] = pd.NaT\n",
    "\t# Se usamos uma coluna temporal principal, sincronizar features derivadas\n",
    "\tif 'main_date_col' in globals() and main_date_col and main_date_col in f.columns:\n",
    "\t\ttry:\n",
    "\t\t\tf[main_date_col] = pd.to_datetime(pd.Series([pd.Timestamp(int(year), 1, 1)] * len(f)))\n",
    "\t\texcept Exception:\n",
    "\t\t\tpass\n",
    "\t# Rótulo do ano previsto\n",
    "\tf['ANO_PREVISTO'] = int(year)\n",
    "\tscenario_frames.append(f)\n",
    "\n",
    "df_future = pd.concat(scenario_frames, ignore_index=True)\n",
    "\n",
    "# Garantir que teremos exatamente as mesmas colunas de entrada do treino\n",
    "for col in [c for c in X.columns if c not in df_future.columns]:\n",
    "\tdf_future[col] = pd.NA\n",
    "\n",
    "# Transformar e prever\n",
    "X_future = df_future[X.columns]\n",
    "X_future_t = preprocessor.transform(X_future)\n",
    "proba_or_pred = model.predict(X_future_t, verbose=0)\n",
    "\n",
    "if TASK_TYPE == 'regression':\n",
    "\ty_pred = proba_or_pred.reshape(-1)\n",
    "\tpred_col_name = 'pred_' + str(TARGET_COLUMN).replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_')\n",
    "\tdf_future_out = df_future.copy()\n",
    "\tdf_future_out[pred_col_name] = y_pred\n",
    "else:\n",
    "\tpred_idx = proba_or_pred.argmax(axis=1)\n",
    "\t# Mapear rótulos se disponível\n",
    "\tlabels = pred_idx\n",
    "\ttry:\n",
    "\t\tfrom pathlib import Path as _P\n",
    "\t\tlabelmaps = sorted((_P(MODEL_DIR)).glob('labelmap_*.json'))\n",
    "\t\tif labelmaps:\n",
    "\t\t\timport json\n",
    "\t\t\twith open(labelmaps[-1], 'r', encoding='utf-8') as f:\n",
    "\t\t\t\tm = json.load(f)\n",
    "\t\t\t\tindex_to_label = {int(k): v for k, v in m.get('index_to_label', {}).items()}\n",
    "\t\t\tlabels = [index_to_label.get(int(i), str(i)) for i in pred_idx]\n",
    "\texcept Exception:\n",
    "\t\tpass\n",
    "\tdf_future_out = df_future.copy()\n",
    "\tdf_future_out['pred_label'] = labels\n",
    "\n",
    "# Selecionar colunas chave para saída, mantendo identificadores úteis\n",
    "id_out_cols = [c for c in ['ANO_PREVISTO', 'ID-ÚNICO-TT', 'ID-ÚNICO', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'UF', 'SETOR', 'PLANILHA'] if c in df_future_out.columns]\n",
    "value_cols = [c for c in df_future_out.columns if c.startswith('pred_') or c == 'pred_label']\n",
    "other_cols = []\n",
    "\n",
    "df_export = df_future_out[id_out_cols + value_cols + other_cols]\n",
    "\n",
    "# Salvar\n",
    "Path(ARTIFACTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "output_path = Path(ARTIFACTS_DIR) / OUTPUT_FUTURE_FILE\n",
    "if output_path.suffix.lower() == '.csv':\n",
    "\tdf_export.to_csv(output_path, index=False)\n",
    "else:\n",
    "\tdf_export.to_excel(output_path, index=False)\n",
    "\n",
    "print('Predições futuras salvas em:', output_path)\n",
    "print('Linhas geradas:', len(df_export))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arquivo wide salvo em: artifacts\\predicoes_futuras_2026-2030_wide.xlsx\n",
      "Colunas preditas: 65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['Ext.(km)2' 'FINANCEIRO PLAN (R$)2' 'FINANCEIRO(R$)2' '(km)% PLAN2'\n",
      " '(km)% EXEC2']. At least one non-missing value is needed for imputation with strategy='median'.\n",
      "  warnings.warn(\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Temp\\ipykernel_34856\\2130103296.py:10: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  return df_in.applymap(lambda v: str(v) if pd.notna(v) else v)\n",
      "C:\\Users\\carlos.ramos\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\impute\\_base.py:653: UserWarning: Skipping features without any observed values: ['SETOR' 'UF' 'Atributo.1' 'Atributo.2' 'Atributo.3' 'Valor' 'SETOR2'\n",
      " 'EMPREENDIMENTO2' 'Descrição2' 'Descrição']. At least one non-missing value is needed for imputation with strategy='most_frequent'.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Previsões multi-métrica (colunas por ano) 2026–2030\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "YEARS_TO_FORECAST = [2026, 2027, 2028, 2029, 2030]\n",
    "MULTI_TARGETS = [\n",
    "\t'CAPEX (BI)', 'OPEX (BI)', 'INVESTIMENTO TOTAL (BI)',\n",
    "\t'EXTENSÃO (km)', '(km)%', '(km)% EXEC', '(km)% PLAN',\n",
    "\t'Ext. (km)', 'FINANCEIRO (R$)', 'FINANCEIRO PLAN (R$)',\n",
    "\t'PERCENTUAL (%)', 'PERCENTUAL (%) EXEC', 'PERCENTUAL (%) PLAN'\n",
    "]\n",
    "OUTPUT_WIDE_FILE = 'predicoes_futuras_2026-2030_wide.xlsx'\n",
    "\n",
    "assert 'df' in globals() and 'X' in globals(), 'Execute as células de carregamento e de engenharia antes.'\n",
    "assert 'preprocessor' in globals(), 'Execute a célula de pre-processamento/treino.'\n",
    "assert 'X_train_t' in globals() and 'X_train' in globals(), 'Execute a célula de split/transformação antes.'\n",
    "\n",
    "# Filtrar as métricas existentes e numéricas\n",
    "valid_targets = [c for c in MULTI_TARGETS if c in df.columns]\n",
    "valid_targets = [c for c in valid_targets if pd.api.types.is_numeric_dtype(pd.to_numeric(df[c], errors='coerce'))]\n",
    "assert len(valid_targets) > 0, 'Nenhuma métrica válida encontrada nas colunas.'\n",
    "\n",
    "# Treinar modelos rápidos (Ridge) para cada métrica com os mesmos features do treino\n",
    "models_by_target = {}\n",
    "for col in valid_targets:\n",
    "\ty_all = pd.to_numeric(df[col], errors='coerce')\n",
    "\ty_train_other = y_all.loc[X_train.index]\n",
    "\tmask = np.isfinite(y_train_other.values)\n",
    "\tif mask.sum() < 5:\n",
    "\t\tcontinue\n",
    "\treg = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "\treg.fit(X_train_t[mask], y_train_other.values[mask])\n",
    "\tmodels_by_target[col] = reg\n",
    "\n",
    "assert len(models_by_target) > 0, 'Não foi possível treinar modelos para as métricas selecionadas.'\n",
    "\n",
    "# Reusar df_future da célula anterior ou reconstruir se não existir\n",
    "if 'df_future' not in globals():\n",
    "\t# Reconstruir rapidamente cenários (mesma lógica da célula anterior, simplificada)\n",
    "\tid_candidates = ['ID-ÚNICO-TT', 'ID-ÚNICO', 'ID-ÚNICO2', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'PLANILHA']\n",
    "\tid_col = next((c for c in id_candidates if c in df.columns), None)\n",
    "\tif id_col is not None:\n",
    "\t\tbase_df = df.dropna(subset=[id_col]).copy()\n",
    "\t\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\t\tbase_df = base_df.sort_values(by=main_date_col)\n",
    "\t\t\tbase_df = base_df.groupby(id_col, as_index=False).tail(1)\n",
    "\t\telse:\n",
    "\t\t\tbase_df = base_df.drop_duplicates(subset=[id_col], keep='last')\n",
    "\telse:\n",
    "\t\tbase_df = df.copy()\n",
    "\t\tif 'main_date_col' in globals() and main_date_col and main_date_col in base_df.columns:\n",
    "\t\t\tbase_df = base_df.sort_values(by=main_date_col).tail(1)\n",
    "\n",
    "\tscenario_frames = []\n",
    "\tfor year in YEARS_TO_FORECAST:\n",
    "\t\tf = base_df.copy()\n",
    "\t\tif 'ANO LEILÃO' in f.columns:\n",
    "\t\t\tf['ANO LEILÃO'] = int(year)\n",
    "\t\tif 'ANO DA CONCESSÃO' in f.columns:\n",
    "\t\t\tf['ANO DA CONCESSÃO'] = int(year)\n",
    "\t\tif 'DATA DE INÍCIO' in f.columns:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tf['DATA DE INÍCIO'] = pd.Timestamp(int(year), 1, 1)\n",
    "\t\t\texcept Exception:\n",
    "\t\t\t\tf['DATA DE INÍCIO'] = pd.NaT\n",
    "\t\tif 'main_date_col' in globals() and main_date_col and main_date_col in f.columns:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tf[main_date_col] = pd.to_datetime(pd.Series([pd.Timestamp(int(year), 1, 1)] * len(f)))\n",
    "\t\t\texcept Exception:\n",
    "\t\t\t\tpass\n",
    "\t\tf['ANO_PREVISTO'] = int(year)\n",
    "\t\tscenario_frames.append(f)\n",
    "\t\n",
    "\tdf_future = pd.concat(scenario_frames, ignore_index=True)\n",
    "\tfor col in [c for c in X.columns if c not in df_future.columns]:\n",
    "\t\tdf_future[col] = pd.NA\n",
    "\n",
    "# Predizer por ano e construir colunas wide\n",
    "wide_parts = []\n",
    "for year in YEARS_TO_FORECAST:\n",
    "\tmask_year = df_future['ANO_PREVISTO'] == int(year)\n",
    "\tdf_year = df_future.loc[mask_year].copy()\n",
    "\tX_year_t = preprocessor.transform(df_year[X.columns])\n",
    "\tcols_pred = {}\n",
    "\tfor col, mdl in models_by_target.items():\n",
    "\t\tpred_vals = mdl.predict(X_year_t)\n",
    "\t\tcol_name = 'pred_' + str(col).replace(' ', '_').replace('(', '').replace(')', '').replace('/', '_') + '_' + str(year)\n",
    "\t\tcols_pred[col_name] = pred_vals\n",
    "\tpart = df_year.copy()\n",
    "\tfor k, v in cols_pred.items():\n",
    "\t\tpart[k] = v\n",
    "\twide_parts.append(part)\n",
    "\n",
    "wide_all = pd.concat(wide_parts, ignore_index=True)\n",
    "\n",
    "# Manter uma linha por ID com colunas por ano\n",
    "id_out_cols = [c for c in ['ID-ÚNICO-TT', 'ID-ÚNICO', 'EMPREENDIMENTO', 'ESTADO/LOTE', 'UF', 'SETOR', 'PLANILHA'] if c in wide_all.columns]\n",
    "key_cols = id_out_cols if id_out_cols else X.columns[:1].tolist()\n",
    "value_cols = [c for c in wide_all.columns if c.startswith('pred_')]\n",
    "\n",
    "wide_export = wide_all[key_cols + value_cols].copy()\n",
    "wide_export = wide_export.drop_duplicates(subset=key_cols, keep='first')\n",
    "\n",
    "# Salvar\n",
    "Path(ARTIFACTS_DIR).mkdir(exist_ok=True, parents=True)\n",
    "output_path = Path(ARTIFACTS_DIR) / OUTPUT_WIDE_FILE\n",
    "try:\n",
    "\twide_export.to_excel(output_path, index=False, engine='openpyxl')\n",
    "except PermissionError:\n",
    "\timport time\n",
    "\talt_name = output_path.with_stem(output_path.stem + '_' + time.strftime('%Y%m%d-%H%M%S'))\n",
    "\twide_export.to_excel(alt_name, index=False, engine='openpyxl')\n",
    "\toutput_path = alt_name\n",
    "\n",
    "print('Arquivo wide salvo em:', output_path)\n",
    "print('Colunas preditas:', len(value_cols))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
